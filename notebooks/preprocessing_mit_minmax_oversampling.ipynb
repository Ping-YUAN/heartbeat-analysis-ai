{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing for MIT data(Binary classification) with MinMax Scaler and Oversamling\n",
    "The default preprocessing step is what we conclude in our report. \n",
    "You can check below what should be input for this notebook and what would be the output if necessary.\n",
    "\n",
    "\n",
    "Processing **steps** for MIT dataset:   \n",
    "    resample: Oversampling  \n",
    "    rescaling: MinMaxScaler  \n",
    "\n",
    "\n",
    "**Input** : Cleaned data with dummy target variable.   \n",
    "mitbih_test.csv   \n",
    "mitbih_train.csv\n",
    "\n",
    "**Output** : Sampled and Scaled data with dummy target:   \n",
    "mitbih_train_clean_minmax_oversampling.csv  \n",
    "mitbih_test_clean_minmax_oversampling.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local environment\n",
      "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "data_path = ''\n",
    "data_output_path = ''\n",
    "# Check if the environment is Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    data_output_path = data_path\n",
    "    \n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/raw/'\n",
    "    data_output_path = '../data/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m_FSL5cYnEQI"
   },
   "outputs": [],
   "source": [
    "# Verify installation and import libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'test': data_path +  'mitbih_test.csv', # MIT data set in which we have dummy-coded the target variable\n",
    "    'train': data_path +  'mitbih_train.csv', # MIT data set in which we have dummy-coded the target variable\n",
    "})\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'test': data_output_path + 'mitbih_test_clean_minmax_oversampling.csv',\n",
    "    'train': data_output_path + 'mitbih_train_clean_minmax_oversampling.csv',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hZhtCq2NnEQJ"
   },
   "outputs": [],
   "source": [
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_182'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "def convertColumnAsInt(df, column):\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce') # convert to numeric to handle NaN values\n",
    "    df.dropna(subset=[column], inplace=True)  # drop the rows with NaN values\n",
    "    df[column] = df[column].astype(int)  # convert to int\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isabell Gurstein\\AppData\\Local\\Temp\\ipykernel_22680\\781607896.py:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mitbih_train['target'] = mitbih_train['target'].replace({'Normal': 0, 'abnormal': 1})\n",
      "C:\\Users\\Isabell Gurstein\\AppData\\Local\\Temp\\ipykernel_22680\\781607896.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mitbih_test['target'] = mitbih_test['target'].replace({'Normal': 0, 'abnormal': 1})\n"
     ]
    }
   ],
   "source": [
    "mitbih_train = pd.read_csv(RawFiles.get('train'), header=None ) \n",
    "mitbih_test = pd.read_csv(RawFiles.get('test'), header=None )\n",
    "\n",
    "mitbih_train = addColumnsToDataframe(mitbih_train)\n",
    "mitbih_train = convertColumnAsInt(mitbih_train, 'target')\n",
    "\n",
    "mitbih_test = addColumnsToDataframe(mitbih_test)\n",
    "mitbih_test = convertColumnAsInt(mitbih_test, 'target')\n",
    "\n",
    "# target value and meanings\n",
    "all_class_mapping = {\n",
    "    0: 'Normal',\n",
    "    1: 'Supraventricular',\n",
    "    2: 'Ventricular',\n",
    "    3: 'Fusion',\n",
    "    4: 'Unclassifiable'\n",
    "}\n",
    "mitbih_train['target'] = mitbih_train['target'].map(all_class_mapping)\n",
    "mitbih_test['target'] = mitbih_test['target'].map(all_class_mapping)\n",
    "# Drop rows where 'target' is 'Unclassifiable beat'\n",
    "mitbih_train = mitbih_train[mitbih_train['target'] != 'Unclassifiable']\n",
    "mitbih_test = mitbih_test[mitbih_test['target'] != 'Unclassifiable']\n",
    "\n",
    "\n",
    "# convert to binary classification Combine abnormal categories\n",
    "mitbih_train['target'] = mitbih_train['target'].replace(['Supraventricular', 'Ventricular', 'Fusion'], 'abnormal')\n",
    "mitbih_test['target'] = mitbih_test['target'].replace(['Supraventricular', 'Ventricular', 'Fusion'], 'abnormal')\n",
    "\n",
    "# Encode the labels: normal as 0, abnormal as 1\n",
    "mitbih_train['target'] = mitbih_train['target'].replace({'Normal': 0, 'abnormal': 1})\n",
    "mitbih_test['target'] = mitbih_test['target'].replace({'Normal': 0, 'abnormal': 1})\n",
    "\n",
    "# target value and meanings\n",
    "# class_mapping = {\n",
    "#     0: 'Normal',\n",
    "#     1: 'Abnormal'\n",
    "# }\n",
    "\n",
    "#drop null value  \n",
    "mitbih_train = mitbih_train.dropna(how='any')\n",
    "mitbih_test = mitbih_test.dropna(how='any')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    72471\n",
       "1     8652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mitbih_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    18118\n",
       "1     2166\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mitbih_test[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test set before resampling\n",
    "\n",
    "X_train = mitbih_train.drop(columns=['target'], inplace=False) # drop the target column\n",
    "X_test = mitbih_test.drop(columns=['target'], inplace=False) # drop the target column\n",
    "\n",
    "y_train = mitbih_train['target'] # only the target column\n",
    "y_test = mitbih_test['target'] # only the target column\n",
    "\n",
    "\n",
    "# Resampling the train data with Oversampling before scaling with MinMaxScaler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert resampling rescaling data back to dataframe to concat\n",
    "X_train_scaled_df =  pd.DataFrame(X_train_scaled, columns=[f'c_{i}' for i in range(X_train_scaled.shape[1])])\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f'c_{i}' for i in range(X_test_scaled.shape[1])])\n",
    "y_train_resampled_df = pd.DataFrame(y_train_resampled, columns=['target'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['target']) \n",
    "\n",
    "\n",
    "# concat X_train, y_train/ X_test, y_test\n",
    "mitbih_train_clean_default = pd.concat(\n",
    "    [\n",
    "        X_train_scaled_df,\n",
    "        y_train_resampled_df\n",
    "    ], axis=1)\n",
    "\n",
    "mitbih_test_clean_default = pd.concat(\n",
    "    [  \n",
    "        X_test_scaled_df,\n",
    "        y_test_df.reset_index(drop=True)\n",
    "    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save clean data to \n",
    "# mitbih_train_clean_minmax_oversampling.csv  \n",
    "# mitbih_test_clean_minmax_oversampling.csv\n",
    "\n",
    "mitbih_train_clean_default.to_csv(OutputFiles.get('train'), index=False)\n",
    "mitbih_test_clean_default.to_csv(OutputFiles.get('test'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2024-11-13 10:06:11.105349\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
