{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Preprocessing for MIT data with MinMax Scaler and Oversamling\n",
        "The default preprocessing step is what we conclude in our report. \n",
        "You can check below what should be input for this notebook and what would be the output if necessary.\n",
        "\n",
        "Processing **steps** for MIT dataset:   \n",
        "    resample: Oversampling  \n",
        "    rescaling: MinMaxScaler  \n",
        "\n",
        "\n",
        "**Input** : Cleaned data with dummy target variable.   \n",
        "mitbih_test_clean.csv   \n",
        "mitbih_train_clean.csv\n",
        "\n",
        "**Output** : Sampled and Scaled data with dummy target:   \n",
        "mitbih_train_clean_minmax_oversampling.csv  \n",
        "mitbih_test_clean_minmax_oversampling.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local environment\n",
            "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "data_path = ''\n",
        "data_output_path = ''\n",
        "# Check if the environment is Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running on Google Colab\")\n",
        "    # Install required libraries\n",
        "    !pip install scikit-learn -q\n",
        "    !pip install pandas -q\n",
        "    !pip install numpy -q\n",
        "    !pip install imbalanced-learn -q\n",
        "\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # set the path where the csv file stored in your google drive. \n",
        "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
        "    data_output_path = data_path\n",
        "    \n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "\n",
        "    current_path = os.getcwd()\n",
        "    print(\"Current working directory:\", current_path)\n",
        "    data_path = '../data/raw/'\n",
        "    data_output_path = '../data/processed/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m_FSL5cYnEQI"
      },
      "outputs": [],
      "source": [
        "# Verify installation and import libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "RawFiles = dict({\n",
        "    'test': data_path +  'mitbih_test_clean.csv', # MIT data set in which we have dummy-coded the target variable\n",
        "    'train': data_path +  'mitbih_train_clean.csv', # MIT data set in which we have dummy-coded the target variable\n",
        "})\n",
        "\n",
        "OutputFiles = dict({\n",
        "    'test': data_output_path + 'mitbih_test_clean_minmax_oversamling.csv',\n",
        "    'train': data_output_path + 'mitbih_train_clean_minmax_oversampling.csv',\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hZhtCq2NnEQJ"
      },
      "outputs": [],
      "source": [
        "def addColumnsToDataframe(df):\n",
        "    \"\"\"\n",
        "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
        "    so we give the last column the name 'target', others named with 'c_182'\n",
        "    \"\"\"\n",
        "    num_columns= df.shape[1]\n",
        "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
        "    df_columns = feature_col_name + ['target']\n",
        "    df.columns = df_columns\n",
        "    return df\n",
        "\n",
        "# def convertColumnAsInt(df, column):\n",
        "#     \"\"\"\n",
        "#     As the category value is in float type. We want to get the int to identify the category.\n",
        "#     \"\"\"\n",
        "#     df[column] = df[column].astype(int)\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convertColumnAsInt(df, column):\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce') # convert to numeric to handle NaN values\n",
        "    df.dropna(subset=[column], inplace=True)  # drop the rows with NaN values\n",
        "    df[column] = df[column].astype(int)  # convert to int\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Isabell Gurstein\\AppData\\Local\\Temp\\ipykernel_42316\\433990335.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  mitbih_train = pd.read_csv(RawFiles.get('train'), header=None )\n",
            "C:\\Users\\Isabell Gurstein\\AppData\\Local\\Temp\\ipykernel_42316\\433990335.py:2: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  mitbih_test = pd.read_csv(RawFiles.get('test'), header=None )\n"
          ]
        }
      ],
      "source": [
        "mitbih_train = pd.read_csv(RawFiles.get('train'), header=None ) \n",
        "mitbih_test = pd.read_csv(RawFiles.get('test'), header=None )\n",
        "\n",
        "mitbih_train = addColumnsToDataframe(mitbih_train)\n",
        "mitbih_train = convertColumnAsInt(mitbih_train, 'target')\n",
        "\n",
        "mitbih_test = addColumnsToDataframe(mitbih_test)\n",
        "mitbih_test = convertColumnAsInt(mitbih_test, 'target')\n",
        "\n",
        "# target value and meanings\n",
        "class_mapping = {\n",
        "    0: 'Normal',\n",
        "    1: 'Abnormal'\n",
        "}\n",
        "\n",
        "#drop null value  \n",
        "mitbih_train = mitbih_train.dropna(how='any')\n",
        "mitbih_test = mitbih_test.dropna(how='any')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    72471\n",
              "1     8652\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mitbih_train[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    18118\n",
              "1     2166\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mitbih_test[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split train test set before resampling\n",
        "\n",
        "X_train = mitbih_train.drop(columns=['target'], inplace=False) # drop the target column\n",
        "X_test = mitbih_test.drop(columns=['target'], inplace=False) # drop the target column\n",
        "\n",
        "y_train = mitbih_train['target'] # only the target column\n",
        "y_test = mitbih_test['target'] # only the target column\n",
        "\n",
        "\n",
        "# Resampling the train data with Oversampling before scaling with MinMaxScaler\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Scaling with MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert resampling rescaling data back to dataframe to concat\n",
        "X_train_scaled_df =  pd.DataFrame(X_train_scaled, columns=[f'c_{i}' for i in range(X_train_scaled.shape[1])])\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f'c_{i}' for i in range(X_test_scaled.shape[1])])\n",
        "y_train_resampled_df = pd.DataFrame(y_train_resampled, columns=['target'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['target']) \n",
        "\n",
        "\n",
        "# concat X_train, y_train/ X_test, y_test\n",
        "mitbih_train_clean_default = pd.concat(\n",
        "    [\n",
        "        X_train_scaled_df,\n",
        "        y_train_resampled_df\n",
        "    ], axis=1)\n",
        "\n",
        "mitbih_test_clean_default = pd.concat(\n",
        "    [  \n",
        "        X_test_scaled_df,\n",
        "        y_test_df.reset_index(drop=True)\n",
        "    ], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save clean data to \n",
        "# mitbih_train_clean_default.csv\n",
        "# mitbih_test_clean_default.csv\n",
        "\n",
        "mitbih_train_clean_default.to_csv(OutputFiles.get('train'), index=False)\n",
        "mitbih_test_clean_default.to_csv(OutputFiles.get('test'), index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
