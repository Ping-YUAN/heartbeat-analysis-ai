{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and GRU Models for PTB data \n",
    "As we concluded before, for PTB data, we apply the following preprocessing:   \n",
    "resampling: Oversampling  \n",
    "rescaling: Standard\n",
    "\n",
    "If you don't have the original files: run the notebook `preprocessing_ptb_standard_oversampling.ipynb`     \n",
    "Input file:(The preprocessed data)     \n",
    "ptb_train_clean_standard_oversampling.csv (Standard Scaled data)  \n",
    "ptb_test_clean_standard_oversampling.csv  (Standard Sscaled data)   \n",
    "\n",
    "Output: lstm model trained  \n",
    "model_ptb_lstm.h5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Feature**                     | **LSTM**                                                      | **GRU**                                                 |\n",
    "|---------------------------------|---------------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Types of Gates**              | 3 gates: Input, Forget, Output                                | 2 gates: Update, Reset                                  |\n",
    "| **Memory Structure**            | Keeps two kinds of memory: cell and hidden                    | Only one memory type: hidden                            |\n",
    "| **Computational Demand**        | Uses more power and memory                                    | Uses less power and memory                              |\n",
    "| **Training Speed**              | Takes more time to train                                      | Trains faster                                           |\n",
    "| **Best for Long Sequences**     | Great for remembering long sequences and details              | Good for simpler, shorter sequences                     |\n",
    "| **When to Use**                 | When you need to remember a lot of details over a long period | When you want quicker training with fewer details       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab \n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    model_output_path = data_path\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use ImbPipeline for oversampling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'ptb_train_clean_standard_oversampling.csv', \n",
    "    'test': data_path + 'ptb_test_clean_standard_oversampling.csv'  \n",
    "})\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'model_ptb_lstm'\n",
    "})\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Standard Scaler and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to NumPy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Reshape the data to fit the LSTM model (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Check the shape of the data\n",
    "print(\"X_train shape:\", X_train.shape)  # samples, timesteps, features\n",
    "print(\"X_test shape:\", X_test.shape)    # samples, timesteps, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(187, activation='tanh', return_sequences=True))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(LSTM(16, activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Choose an optimizer with gradient clipping\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.01)\n",
    "\n",
    "# Lists to store metrics over epochs\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Training loop on the full dataset\n",
    "for epoch in range(epochs):\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, \n",
    "                        validation_data=(X_test, y_test), verbose=0, \n",
    "                        callbacks=[early_stopping, lr_scheduler])\n",
    "    \n",
    "    # Append metrics to respective lists\n",
    "    train_loss.append(history.history['loss'][0])\n",
    "    val_loss.append(history.history['val_loss'][0])\n",
    "    train_accuracy.append(history.history['accuracy'][0])\n",
    "    val_accuracy.append(history.history['val_accuracy'][0])\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}, \"\n",
    "              f\"Accuracy: {train_accuracy[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Predictions and optimal threshold calculation on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "\n",
    "# Convert probabilities to binary predictions based on optimal threshold\n",
    "y_pred_class_test = (y_pred_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# Model Evaluation on the test set\n",
    "print(\"Evaluation on Test Set\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class_test))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class_test))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_class_test):.4f}\")\n",
    "\n",
    "# Predictions on the full training set (X_train) using the optimal threshold\n",
    "y_pred_full_train = model.predict(X_train)\n",
    "y_pred_class_full_train = (y_pred_full_train >= optimal_threshold).astype(int)\n",
    "\n",
    "# Model Evaluation on the full training set\n",
    "print(\"\\nEvaluation on Full Training Set\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred_class_full_train))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred_class_full_train))\n",
    "print(f\"F1 Score: {f1_score(y_train, y_pred_class_full_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Plotting the Loss\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Model Loss by Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Accuracy\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy by Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions using the best model (with best weights)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)  # Using 0.5 as the threshold for binary classification\n",
    "\n",
    "# Evaluating the best-performing model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_class):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with TensorFlow format\n",
    "model_path = OutputFiles.get('model') + '.h5'  # Append .h5 extension\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
