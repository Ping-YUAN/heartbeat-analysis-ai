{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebooks is used to find the best scaler method and resampling method for MIT dataset. \n",
    "\n",
    "We will perform several base machine learning model then evaluate the model with f1 score to find the best one. \n",
    "\n",
    "We will perform two simulations: \n",
    "1. for binary classificaion  \n",
    "2. for multiple classification \n",
    "\n",
    "Conclusion: \n",
    "| | rescaling | resampling | \n",
    "| --- | --- | --- |\n",
    "| Binary | minmax | oversampling |\n",
    "| Multiple | minmax | oversampling  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_0 to c_186'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "\n",
    "def convertColumnAsInt(df, column):\n",
    "    \"\"\"\n",
    "    As the category value is in float type. We want to get the int to identify the category.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitbih_train_raw = pd.read_csv('../data/raw/mitbih_train.csv', header=None)\n",
    "mitbih_test_raw = pd.read_csv('../data/raw/mitbih_test.csv', header=None)\n",
    "\n",
    "mitbih_train_with_column = addColumnsToDataframe(mitbih_train_raw)\n",
    "mitbih_test_with_column = addColumnsToDataframe(mitbih_test_raw)\n",
    "\n",
    "mitbih_train_label_target = convertColumnAsInt(mitbih_train_with_column, 'target')\n",
    "mitbih_test_label_target = convertColumnAsInt(mitbih_test_with_column, 'target')\n",
    "\n",
    "\n",
    "# target value and meanings\n",
    "all_class_mapping = {\n",
    "    0: 'Normal',\n",
    "    1: 'Supraventricular',\n",
    "    2: 'Ventricular',\n",
    "    3: 'Fusion',\n",
    "    4: 'Unclassifiable'\n",
    "}\n",
    "\n",
    "mitbih_train_label_target['target'] = mitbih_train_label_target['target'].map(all_class_mapping)\n",
    "mitbih_test_label_target['target'] = mitbih_test_label_target['target'].map(all_class_mapping)\n",
    "\n",
    "# drop unclassifiable\n",
    "mitbih_train_label_target = mitbih_train_label_target[mitbih_train_label_target['target'] != 'Unclassifiable']\n",
    "mitbih_test_label_target = mitbih_test_label_target[mitbih_test_label_target['target'] != 'Unclassifiable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/y9wp4lwn1zs33k6r9j_jn10c0000gn/T/ipykernel_89913/317402102.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mit_binary_train['target'] = mit_binary_train['target'].replace({'Normal': 0, 'abnormal': 1})\n",
      "/var/folders/7l/y9wp4lwn1zs33k6r9j_jn10c0000gn/T/ipykernel_89913/317402102.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mit_binary_test['target'] = mit_binary_test['target'].replace({'Normal': 0, 'abnormal': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    72471\n",
       "1     8652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate dataframe for binary classification\n",
    "\n",
    "mit_binary_train = mitbih_train_label_target.copy()\n",
    "mit_binary_test = mitbih_test_label_target.copy()\n",
    "\n",
    "\n",
    "# convert to binary classification Combine abnormal categories\n",
    "mit_binary_train['target'] = mit_binary_train['target'].replace(['Supraventricular', 'Ventricular', 'Fusion'], 'abnormal')\n",
    "mit_binary_test['target'] = mit_binary_test['target'].replace(['Supraventricular', 'Ventricular', 'Fusion'], 'abnormal')\n",
    "\n",
    "# Encode the labels: normal as 0, abnormal as 1\n",
    "mit_binary_train['target'] = mit_binary_train['target'].replace({'Normal': 0, 'abnormal': 1})\n",
    "mit_binary_test['target'] = mit_binary_test['target'].replace({'Normal': 0, 'abnormal': 1})\n",
    "\n",
    "\n",
    "mit_binary_train = mit_binary_train.dropna()\n",
    "mit_binary_test = mit_binary_test.dropna()\n",
    "\n",
    "mit_binary_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "Normal              72471\n",
       "Ventricular          5788\n",
       "Supraventricular     2223\n",
       "Fusion                641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate dataframe for multi classification\n",
    "mit_multi_train = mitbih_train_label_target.copy()\n",
    "mit_multi_test = mitbih_test_label_target.copy()\n",
    "\n",
    "\n",
    "mit_multi_train = mit_multi_train.dropna()\n",
    "mit_multi_test = mit_multi_test.dropna()\n",
    "\n",
    "mit_multi_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classificaiton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary = mit_binary_train.drop( columns=['target'])\n",
    "y_train_binary = mit_binary_train['target']\n",
    "\n",
    "X_test_binary = mit_binary_test.drop(columns=['target'])\n",
    "y_test_binary = mit_binary_test['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the best rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.18, 0.46, 0.5, 0.48, 0.42]\n",
      "\n",
      "F1-Score mean=0.40887\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.66, 0.83, 0.85, 0.85, 0.72]\n",
      "\n",
      "F1-Score mean=0.78168\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.89, 0.91, 0.92, 0.77]\n",
      "\n",
      "F1-Score mean=0.84705\n",
      "\n",
      "Scaler: MinMaxScaler\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Apply evaluation\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_scalers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Summarize the results\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_name, model_scores \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 45\u001b[0m, in \u001b[0;36mevaluate_scalers\u001b[0;34m(X, y, scalers, models)\u001b[0m\n\u001b[1;32m     42\u001b[0m X_train_, y_train_ \u001b[38;5;241m=\u001b[39m X_scaled[train_index], y[train_index]\n\u001b[1;32m     43\u001b[0m X_test_, y_test_ \u001b[38;5;241m=\u001b[39m X_scaled[test_index], y[test_index]\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m y_pred_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_)\n\u001b[1;32m     49\u001b[0m f_score\u001b[38;5;241m.\u001b[39mappend(f1_score(y_test_, y_pred_))\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[1;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    454\u001b[0m ]\n\u001b[0;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    470\u001b[0m     solver,\n\u001b[1;32m    471\u001b[0m     opt_res,\n\u001b[1;32m    472\u001b[0m     max_iter,\n\u001b[1;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:281\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m--> 281\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m sw_sum \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(sample_weight)\n\u001b[1;32m    288\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m sw_sum\n",
      "File \u001b[0;32m~/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/_loss/loss.py:255\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    253\u001b[0m     gradient_out \u001b[38;5;241m=\u001b[39m gradient_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_out, gradient_out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# define general used scalers.\n",
    "scalers = { \n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "    \"None\": None,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "     # \"SVM\": SVC(class_weight='balanced', probability=True, random_state=42), # SVM is computationally too expensive, after 80 minutes, it was still running!!!\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "}\n",
    "\n",
    "def evaluate_scalers(X, y, scalers, models):\n",
    "    results = {}\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"Scaler: {scaler_name}\", end=\"\\n\\n\")\n",
    "\n",
    "        if scaler is None: \n",
    "            X_scaled = X.to_numpy()\n",
    "        else:\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            f_score = []\n",
    "            print(f\"Model: {model_name}\", end=\"\\n\\n\")\n",
    "\n",
    "            for train_index, test_index in skf.split(X_scaled, y):\n",
    "                X_train_, y_train_ = X_scaled[train_index], y[train_index]\n",
    "                X_test_, y_test_ = X_scaled[test_index], y[test_index]\n",
    "\n",
    "                model.fit(X_train_, y_train_)\n",
    "\n",
    "                y_pred_ = model.predict(X_test_)\n",
    "\n",
    "                f_score.append(f1_score(y_test_, y_pred_))\n",
    "\n",
    "            mean_f1_score = np.mean(f_score)\n",
    "            print(\"The scores: \", end=\"\\n\\n\")\n",
    "            print([round(f, 2) for f in f_score], end=\"\\n\\n\")\n",
    "            print('F1-Score mean=%.5f' % (mean_f1_score), end=\"\\n\\n\")\n",
    "\n",
    "            if scaler_name not in results:\n",
    "                results[scaler_name] = {}\n",
    "            results[scaler_name][model_name] = mean_f1_score\n",
    "    return results\n",
    "\n",
    "# Apply evaluation\n",
    "results = evaluate_scalers(X_train_binary, y_train_binary, scalers, models)\n",
    "# Summarize the results\n",
    "for scaler_name, model_scores in results.items():\n",
    "    print(f\"Scaler: {scaler_name}\")\n",
    "    for model_name, score in model_scores.items():\n",
    "        print(f\"  Model: {model_name}, F1-Score mean: {score:.5f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get result: \n",
    "\n",
    "| Model | StandardScaler | MinMaxScaler | RobustScaler | None |\n",
    "| --- | --- | --- | --- | ---- | \n",
    "| LogisticRegression | 0.40887 | 0.40801 |  0.40920 | 0.40801 |\n",
    "| Decision Tree |  0.78168 | 0.78168 | 0.78168 | 0.78168 | \n",
    "| KNN | 0.84705 | 0.85536 | 0.84558 | 0.85536 | \n",
    "\n",
    "\n",
    "From the table:\n",
    "RobustScaler wins for LogisticRegression by a narrow margin.     \n",
    "Decision Tree got all equal result which is logic because it doesn't require scaler.     \n",
    "MinmaxScaler/None scaler win for KNN with a little advantage.   \n",
    "\n",
    "As KNN is adapted for anomaly detection, which is more like the cases that we will treat. We can count more important according to KNN result. \n",
    "\n",
    "To conclude: MinMaxScaler could be better for us. Even in certain model it is not the best but the margin is very narrow. We can accept that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply minmax scaler \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_binary_scaled = scaler.fit_transform(X_train_binary)\n",
    "X_test_binary_scaled = scaler.transform(X_test_binary)\n",
    "\n",
    "X_train_binary_scaled = pd.DataFrame(X_train_binary_scaled, columns=X_train_binary.columns)\n",
    "X_test_binary_scaled = pd.DataFrame(X_test_binary_scaled, columns=X_test_binary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best resampling method with the best scaler we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.19, 0.46, 0.5, 0.47, 0.42]\n",
      "\n",
      "F1-Score mean=0.40885\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.66, 0.79, 0.8, 0.8, 0.71]\n",
      "\n",
      "F1-Score mean=0.75390\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.76, 0.85, 0.87, 0.87, 0.77]\n",
      "\n",
      "F1-Score mean=0.82550\n",
      "\n",
      "Oversampling\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.18, 0.46, 0.5, 0.48, 0.42]\n",
      "\n",
      "F1-Score mean=0.40727\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.68, 0.82, 0.85, 0.85, 0.72]\n",
      "\n",
      "F1-Score mean=0.78438\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.77, 0.87, 0.88, 0.89, 0.77]\n",
      "\n",
      "F1-Score mean=0.83522\n",
      "\n",
      "Undersampling\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.15, 0.46, 0.5, 0.49, 0.43]\n",
      "\n",
      "F1-Score mean=0.40558\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.58, 0.64, 0.65, 0.65, 0.57]\n",
      "\n",
      "F1-Score mean=0.62156\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.69, 0.78, 0.79, 0.8, 0.68]\n",
      "\n",
      "F1-Score mean=0.74698\n",
      "\n",
      "NONE\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.17, 0.47, 0.5, 0.48, 0.42]\n",
      "\n",
      "F1-Score mean=0.40801\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.66, 0.83, 0.85, 0.85, 0.72]\n",
      "\n",
      "F1-Score mean=0.78168\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.9, 0.93, 0.93, 0.78]\n",
      "\n",
      "F1-Score mean=0.85536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    #\"SVM\": SVC(class_weight='balanced', probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "}\n",
    "\n",
    "def crossvalidation(X, y, models):\n",
    "    resampling_methods = {\n",
    "        \"SMOTE\": SMOTE(),\n",
    "        \"Oversampling\": RandomOverSampler(sampling_strategy='not majority'),\n",
    "        \"Undersampling\": RandomUnderSampler(sampling_strategy='majority'),\n",
    "        \"NONE\": None\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    results = {}\n",
    "\n",
    "    for name, resample in resampling_methods.items():\n",
    "        print(name, end=\"\\n\\n\")\n",
    "        results[name] = {}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            f_score = []\n",
    "            print(f\"Model: {model_name}\", end=\"\\n\\n\")\n",
    "\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train_, y_train_ = X.loc[train_index], y.loc[train_index]\n",
    "                X_test_, y_test_ = X.loc[test_index], y.loc[test_index]\n",
    "\n",
    "                if name == \"BalancedRandomForest\" or name == 'NONE':\n",
    "                    model.fit(X_train_, y_train_)\n",
    "                else:\n",
    "                    X_train_resampled, y_train_resampled = resample.fit_resample(X_train_, y_train_)\n",
    "                    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "                y_pred_ = model.predict(X_test_)\n",
    "\n",
    "                f_score.append(f1_score(y_test_, y_pred_))\n",
    "\n",
    "            results[name][model_name] = np.mean(f_score)\n",
    "            print(\"The scores: \", end=\"\\n\\n\")\n",
    "            print([round(f, 2) for f in f_score], end=\"\\n\\n\")\n",
    "            print('F1-Score mean=%.5f' % (np.mean(f_score)), end=\"\\n\\n\")\n",
    "\n",
    "    return results, resampling_methods\n",
    "\n",
    "\n",
    "\n",
    "results_resample, resampling_methods = crossvalidation(X_train_binary_scaled, y_train_binary, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get result: \n",
    "\n",
    "| Model | SMOTE | Oversampling | Undersampling | None |\n",
    "| --- | --- | --- | --- | ---- | \n",
    "| LogisticRegression | 0.40885 | 0.40727 | 0.40558  | 0.40801 |\n",
    "| Decision Tree |  0.75390 | 0.78438 | 0.62156 | 0.78168 | \n",
    "| KNN | 0.82550 | 0.83522 | 0.74698 | 0.85536 | \n",
    "\n",
    "\n",
    "From the table:   \n",
    "SMOTE wins for LogisticRegression by a narrow margin.        \n",
    "Oversampling wins for Decision Tree   \n",
    "None scaler win for KNN.  \n",
    "\n",
    "\n",
    "To conclude: Oversampling could be better for us. Even in certain model it is not the best but the margin is very narrow. We can accept that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_multi = mit_multi_train.drop(columns=['target'])\n",
    "y_train_multi = mit_multi_train['target']\n",
    "\n",
    "X_test_multi = mit_multi_test.drop(columns=['target'])\n",
    "y_test_multi = mit_multi_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the best rescaling method for multiple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.74037\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95981\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.98, 0.97, 0.97, 0.97]\n",
      "\n",
      "F1-Score mean=0.97421\n",
      "\n",
      "Scaler: MinMaxScaler\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.73981\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95981\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.98, 0.98, 0.97, 0.98]\n",
      "\n",
      "F1-Score mean=0.97559\n",
      "\n",
      "Scaler: RobustScaler\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.74129\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95981\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.97, 0.97, 0.97, 0.97]\n",
      "\n",
      "F1-Score mean=0.97366\n",
      "\n",
      "Scaler: None\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.73981\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95981\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.98, 0.98, 0.97, 0.98]\n",
      "\n",
      "F1-Score mean=0.97559\n",
      "\n",
      "Scaler: StandardScaler\n",
      "  Model: LogisticRegression, F1-Score mean: 0.74037\n",
      "  Model: Tree, F1-Score mean: 0.95981\n",
      "  Model: KNN, F1-Score mean: 0.97421\n",
      "\n",
      "\n",
      "Scaler: MinMaxScaler\n",
      "  Model: LogisticRegression, F1-Score mean: 0.73981\n",
      "  Model: Tree, F1-Score mean: 0.95981\n",
      "  Model: KNN, F1-Score mean: 0.97559\n",
      "\n",
      "\n",
      "Scaler: RobustScaler\n",
      "  Model: LogisticRegression, F1-Score mean: 0.74129\n",
      "  Model: Tree, F1-Score mean: 0.95981\n",
      "  Model: KNN, F1-Score mean: 0.97366\n",
      "\n",
      "\n",
      "Scaler: None\n",
      "  Model: LogisticRegression, F1-Score mean: 0.73981\n",
      "  Model: Tree, F1-Score mean: 0.95981\n",
      "  Model: KNN, F1-Score mean: 0.97559\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# define general used scalers.\n",
    "scalers = { \n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "    \"None\": None,\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "     # \"SVM\": SVC(class_weight='balanced', probability=True, random_state=42), # SVM is computationally too expensive, after 80 minutes, it was still running!!!\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "}\n",
    "\n",
    "def evaluate_scalers(X, y, scalers, models):\n",
    "    results = {}\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"Scaler: {scaler_name}\", end=\"\\n\\n\")\n",
    "\n",
    "        if scaler is None: \n",
    "            X_scaled = X.to_numpy()\n",
    "        else:\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            f_score = []\n",
    "            print(f\"Model: {model_name}\", end=\"\\n\\n\")\n",
    "\n",
    "            for train_index, test_index in skf.split(X_scaled, y):\n",
    "                X_train_, y_train_ = X_scaled[train_index], y[train_index]\n",
    "                X_test_, y_test_ = X_scaled[test_index], y[test_index]\n",
    "\n",
    "                model.fit(X_train_, y_train_)\n",
    "\n",
    "                y_pred_ = model.predict(X_test_)\n",
    "\n",
    "                f_score.append(f1_score(y_test_, y_pred_, average='weighted'))\n",
    "\n",
    "            mean_f1_score = np.mean(f_score)\n",
    "            print(\"The scores: \", end=\"\\n\\n\")\n",
    "            print([round(f, 2) for f in f_score], end=\"\\n\\n\")\n",
    "            print('F1-Score mean=%.5f' % (mean_f1_score), end=\"\\n\\n\")\n",
    "\n",
    "            if scaler_name not in results:\n",
    "                results[scaler_name] = {}\n",
    "            results[scaler_name][model_name] = mean_f1_score\n",
    "    return results\n",
    "\n",
    "# Apply evaluation\n",
    "results = evaluate_scalers(X_train_multi, y_train_multi, scalers, models)\n",
    "# Summarize the results\n",
    "for scaler_name, model_scores in results.items():\n",
    "    print(f\"Scaler: {scaler_name}\")\n",
    "    for model_name, score in model_scores.items():\n",
    "        print(f\"  Model: {model_name}, F1-Score mean: {score:.5f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get result: \n",
    "\n",
    "| Model | StandardScaler | MinMaxScaler | RobustScaler | None |\n",
    "| --- | --- | --- | --- | ---- | \n",
    "| LogisticRegression | 0.74037 | 0.73981  | 0.74129  | 0.73981 |\n",
    "| Decision Tree | 0.95981  | 0.95981 | 0.95981 |0.95981  | \n",
    "| KNN | 0.97421  | 0.97559 | 0.97366 | 0.97559 | \n",
    "\n",
    "\n",
    "From the table:   \n",
    "RobustScaler wins for LogisticRegression by a narrow margin.        \n",
    "Decision Tree got all equal result which is logic because it doesn't require scaler.        \n",
    "MinmaxScaler/None scaler win for KNN with a little advantage.      \n",
    "\n",
    "As KNN is adapted for anomaly detection, which is more like the cases that we will treat. We can count more important according to KNN result. \n",
    "\n",
    "To conclude: MinMaxScaler could be better for us. Even in certain model it is not the best but the margin is very narrow. We can accept that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply scaler to multiple classification\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_multi_scaled = scaler.fit_transform(X_train_multi)\n",
    "X_test_multi_scaled = scaler.transform(X_test_multi)\n",
    "\n",
    "X_train_multi_scaled = pd.DataFrame(X_train_multi_scaled, columns=X_train_multi.columns)\n",
    "X_test_multi_scaled = pd.DataFrame(X_test_multi_scaled, columns=X_test_multi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.75]\n",
      "\n",
      "F1-Score mean=0.74190\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.95, 0.95, 0.94, 0.94, 0.95]\n",
      "\n",
      "F1-Score mean=0.94544\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95845\n",
      "\n",
      "Oversampling\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.74028\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95991\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.97, 0.97, 0.97, 0.97]\n",
      "\n",
      "F1-Score mean=0.96854\n",
      "\n",
      "Undersampling\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.7, 0.7, 0.69, 0.7, 0.71]\n",
      "\n",
      "F1-Score mean=0.69925\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.66, 0.68, 0.67, 0.68, 0.67]\n",
      "\n",
      "F1-Score mean=0.67426\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.67, 0.67, 0.68, 0.67, 0.66]\n",
      "\n",
      "F1-Score mean=0.66985\n",
      "\n",
      "NONE\n",
      "\n",
      "Model: LogisticRegression\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.74, 0.74, 0.74, 0.74, 0.74]\n",
      "\n",
      "F1-Score mean=0.73981\n",
      "\n",
      "Model: Tree\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.96, 0.96, 0.96, 0.96, 0.96]\n",
      "\n",
      "F1-Score mean=0.95981\n",
      "\n",
      "Model: KNN\n",
      "\n",
      "The scores: \n",
      "\n",
      "[0.97, 0.98, 0.98, 0.97, 0.98]\n",
      "\n",
      "F1-Score mean=0.97559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    #\"SVM\": SVC(class_weight='balanced', probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "}\n",
    "\n",
    "def crossvalidation(X, y, models):\n",
    "    resampling_methods = {\n",
    "        \"SMOTE\": SMOTE(),\n",
    "        \"Oversampling\": RandomOverSampler(sampling_strategy='not majority'),\n",
    "        \"Undersampling\": RandomUnderSampler(sampling_strategy='majority'),\n",
    "        \"NONE\": None\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    results = {}\n",
    "\n",
    "    for name, resample in resampling_methods.items():\n",
    "        print(name, end=\"\\n\\n\")\n",
    "        results[name] = {}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            f_score = []\n",
    "            print(f\"Model: {model_name}\", end=\"\\n\\n\")\n",
    "\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train_, y_train_ = X.loc[train_index], y.loc[train_index]\n",
    "                X_test_, y_test_ = X.loc[test_index], y.loc[test_index]\n",
    "\n",
    "                if name == \"BalancedRandomForest\" or name == 'NONE':\n",
    "                    model.fit(X_train_, y_train_)\n",
    "                else:\n",
    "                    X_train_resampled, y_train_resampled = resample.fit_resample(X_train_, y_train_)\n",
    "                    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "                y_pred_ = model.predict(X_test_)\n",
    "\n",
    "                f_score.append(f1_score(y_test_, y_pred_, average='weighted'))\n",
    "\n",
    "            results[name][model_name] = np.mean(f_score)\n",
    "            print(\"The scores: \", end=\"\\n\\n\")\n",
    "            print([round(f, 2) for f in f_score], end=\"\\n\\n\")\n",
    "            print('F1-Score mean=%.5f' % (np.mean(f_score)), end=\"\\n\\n\")\n",
    "\n",
    "    return results, resampling_methods\n",
    "\n",
    "\n",
    "\n",
    "results_resample, resampling_methods = crossvalidation(X_train_multi_scaled, y_train_multi, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get result: \n",
    "\n",
    "| Model | SMOTE | Oversampling | Undersampling | None |\n",
    "| --- | --- | --- | --- | ---- | \n",
    "| LogisticRegression | 0.74190 | 0.74028 | 0.69925  | 0.73981 |\n",
    "| Decision Tree | 0.94544  | 0.95991 | 0.67426 | 0.95981 | \n",
    "| KNN |0.95845 | 0.96854 | 0.66985 | 0.97559 | \n",
    "\n",
    "\n",
    "From the table:  \n",
    "SMOTE wins for LogisticRegression by a narrow margin compare to Oversampling.       \n",
    "Oversampling wins for Decision Tree  \n",
    "None scaler win for KNN.    \n",
    "\n",
    "To conclude: Oversampling could be better for us. Even in certain model it is not the best but the margin is very narrow. We can accept that. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
