{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG69PMQ26HTq"
   },
   "source": [
    "# LSTM (Multiple) for MIT data\n",
    "As we concluded before, for MIT data, we apply the following preprocessing:   \n",
    "resampling: Oversampling  \n",
    "rescaling: MinMax\n",
    "\n",
    "If you don't have the original files: run the notebook `preprocessing_mit_multiple_minmax_oversampling.ipynb`     \n",
    "\n",
    "Input file:(The preprocessed data) \n",
    "\n",
    "mitbih_train_multipleclass_minmax_oversampling.csv\n",
    "mitbih_test_multipleclass_minmax_oversampling.csv\n",
    "\n",
    "Output: lstm model trained  \n",
    "model_mit_multiple_raw_lstm.h5  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzXaUGjy6HTr"
   },
   "source": [
    "| **Feature**                     | **LSTM**                                                      | **GRU**                                                 |\n",
    "|---------------------------------|---------------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Types of Gates**              | 3 gates: Input, Forget, Output                                | 2 gates: Update, Reset                                  |\n",
    "| **Memory Structure**            | Keeps two kinds of memory: cell and hidden                    | Only one memory type: hidden                            |\n",
    "| **Computational Demand**        | Uses more power and memory                                    | Uses less power and memory                              |\n",
    "| **Training Speed**              | Takes more time to train                                      | Trains faster                                           |\n",
    "| **Best for Long Sequences**     | Great for remembering long sequences and details              | Good for simpler, shorter sequences                     |\n",
    "| **When to Use**                 | When you need to remember a lot of details over a long period | When you want quicker training with fewer details       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHrhfdTv6HTr",
    "outputId": "ad390a04-86aa-405b-fe49-66f5d61fd3c7"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "    !pip install tensorflow\n",
    "    !pip install seaborn\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive.\n",
    "    data_path = '/content/drive/MyDrive/data/'\n",
    "    model_output_path = data_path\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdAlJ1-D6HTs"
   },
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc1iZSDU6HTs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'mitbih_train_multipleclass_minmax_oversampling.csv',\n",
    "    'test': data_path + 'mitbih_test_multipleclass_minmax_oversampling.csv'\n",
    "})\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'model_mit_multiple_raw_lstm' \n",
    "})\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ7RAu9S6HTs"
   },
   "source": [
    "# LSTM with MinMax Scaler and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FE3sc4Mf6HTs",
    "outputId": "983f9844-bb0a-4cf9-9ee1-3b817d4dec72"
   },
   "outputs": [],
   "source": [
    "# Convert DataFrames to NumPy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Reshape the data to fit the LSTM model (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Check the shape of the data\n",
    "print(\"X_train shape:\", X_train.shape)  # samples, timesteps, features\n",
    "print(\"X_test shape:\", X_test.shape)    # samples, timesteps, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "id": "pS70h9U96HTs",
    "outputId": "1a522a9c-35c3-49cb-d58a-7e1001d17f86"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(187, activation='tanh', return_sequences=True))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(LSTM(16, activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer for multi-class classification (4 classes)\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Choose an optimizer with gradient clipping\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "\n",
    "# Compile the model with categorical cross-entropy for multi-class classification\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.01)\n",
    "\n",
    "# Lists to store metrics over epochs\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Training loop on the full dataset\n",
    "for epoch in range(epochs):\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), verbose=0,\n",
    "                        callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "    # Append metrics to respective lists\n",
    "    train_loss.append(history.history['loss'][0])\n",
    "    val_loss.append(history.history['val_loss'][0])\n",
    "    train_accuracy.append(history.history['accuracy'][0])\n",
    "    val_accuracy.append(history.history['val_accuracy'][0])\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}, \"\n",
    "              f\"Accuracy: {train_accuracy[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_class_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "# Model Evaluation on the test set\n",
    "print(\"Evaluation on Test Set\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class_test))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class_test))\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_class_test, average='weighted'):.4f}\")\n",
    "\n",
    "# Predictions on the full training set\n",
    "y_pred_full_train = model.predict(X_train)\n",
    "y_pred_class_full_train = np.argmax(y_pred_full_train, axis=1)\n",
    "\n",
    "# Model Evaluation on the full training set\n",
    "print(\"\\nEvaluation on Full Training Set\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred_class_full_train))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred_class_full_train))\n",
    "print(f\"F1 Score: {f1_score(y_train, y_pred_class_full_train, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QLc-XWD66HTs",
    "outputId": "c61ab589-0d92-4e98-9491-68e9be9150a8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Plotting the Loss\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Model Loss by Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Accuracy\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy by Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions using the best model (with best weights)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)  # Convert probabilities to class predictions for multi-class\n",
    "\n",
    "# Evaluating the best-performing model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_class, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqEX7FY-6HTs"
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__OZ-Hy26HTs",
    "outputId": "48d34975-62df-4ec8-b078-6990cf1caffd"
   },
   "outputs": [],
   "source": [
    "# Save the model with TensorFlow format\n",
    "model_path = OutputFiles.get('model') + '.h5'  # Append .h5 extension\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyFgpWXD6HTs",
    "outputId": "415a37e2-8988-40fb-f251-658b5398cf63"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
