{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for PTB data \n",
    "As we concluded before, for MIT data, we apply the following preprocessing:   \n",
    "resampling: Oversampling \\\n",
    "rescaling: Standard\n",
    "\n",
    "If you don't have the original files: run the notebook `preprocessing_ptb_standard_oversampling.ipynb`     \n",
    "Input file:(The preprocessed data)     \n",
    "ptb_train_clean_standard_oversampling.csv (Standard Scaled data)   \n",
    "ptb_test_clean_standard_oversampling.csv  (Standard Sscaled data)    \n",
    "\n",
    "Output: DNN model trained  \n",
    "model_ptb_cnn.h5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab \n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    model_output_path = data_path\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use ImbPipeline for oversampling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'ptb_train_clean_standard_oversampling.csv', \n",
    "    'test': data_path + 'ptb_test_clean_standard_oversampling.csv'  \n",
    "})\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'model_ptb_cnn' #.pkl'\n",
    "})\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to NumPy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Reshape data for (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Check the shape\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, timesteps, 1)\n",
    "print(\"X_test shape:\", X_test.shape)    # Should be (num_samples, timesteps, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN with Standard Scaler and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Seed \n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(187, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    factor=0.1,\n",
    "    cooldown=4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_learning_rate],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Scores\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "print(f\"Final Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}, \"\n",
    "      f\"Accuracy: {train_accuracy[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test).ravel()  # Flatten y_pred to 1D array\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "y_pred_class = (y_pred >= optimal_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Test\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_class):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Plotting the Loss\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Test Loss')\n",
    "plt.title('Model Loss by Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Accuracy\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Test Accuracy')\n",
    "plt.title('Model Accuracy by Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_class):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in HDF5 format\n",
    "model_path = OutputFiles.get('model') + '.h5'  # Append .h5 extension\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
