{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global and Local Explanation for KNN model on MIT data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local environment\n",
      "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "data_path = ''\n",
    "# Check if the environment is Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install matplotlib -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your own google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    \n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/raw/'\n",
    "\n",
    "Path = dict({\n",
    "    'mit_test': data_path +  'mitbih_test_clean.csv',\n",
    "    'mit_train':  data_path + 'mitbih_train_clean.csv',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_182'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "\n",
    "def convertColumnAsInt(df, column):\n",
    "    \"\"\"\n",
    "    As the category value is in float type. We want to get the int to identify the category.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n",
    "def getBarChartFromCategoryValueCounts(category_value_counts):\n",
    "    \"\"\"\n",
    "    We call the plot over the pandas series object to plot the category count values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar_chart = category_value_counts.plot(kind='bar')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=360)\n",
    "    for i in bar_chart.containers:\n",
    "        bar_chart.bar_label(i, label_type='edge')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def showTop10DataInChart(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xDataAxis = list(range(0, df.shape[1]))\n",
    "    yDataRows = list(df.values[1: 10])\n",
    "    for y in yDataRows:\n",
    "        plt.plot(xDataAxis, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBModel, Booster\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "mit_train = pd.read_csv(Path['mit_train'])\n",
    "mit_test = pd.read_csv(Path['mit_test'])\n",
    "\n",
    "# Remove duplicates\n",
    "mit_train = mit_train.drop_duplicates()\n",
    "mit_test = mit_test.drop_duplicates()\n",
    "\n",
    "# Prepare the data\n",
    "mit_train = addColumnsToDataframe(mit_train)\n",
    "mit_test = addColumnsToDataframe(mit_test)\n",
    "mit_train = convertColumnAsInt(mit_train, 'target')\n",
    "mit_test = convertColumnAsInt(mit_test, 'target')\n",
    "\n",
    "# Split data into features and target\n",
    "X_train = mit_train.drop(columns=['target'])\n",
    "y_train = mit_train['target']\n",
    "X_test = mit_test.drop(columns=['target'])\n",
    "y_test = mit_test['target']\n",
    "\n",
    "# Define the model path\n",
    "current_dir = os.getcwd()\n",
    "model_path = os.path.join(current_dir, '..', 'models', 'model_knn_mit.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'imblearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "# Load the XGBoost model (wrapped inside a pipeline)\n",
    "with open(model_path, 'rb') as file:\n",
    "    pipeline_model = pickle.load(file)\n",
    "\n",
    "# Check the type of the model (pipeline)\n",
    "print(f\"Model type: {type(pipeline_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps: {'scaler': MinMaxScaler(), 'oversampling': RandomOverSampler(), 'model': KNeighborsClassifier(weights='distance')}\n",
      "Extracted model type: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n"
     ]
    }
   ],
   "source": [
    "# Look at the steps of the pipeline\n",
    "print(\"Pipeline steps:\", pipeline_model.named_steps)\n",
    "    \n",
    "# Extract the KNN model from the pipeline\n",
    "knn_model = pipeline_model.named_steps['model']\n",
    "\n",
    "# Verify the model type\n",
    "print(\"Extracted model type:\", type(knn_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is an instance-based, non-parametric algorithm that makes predictions based on the distances to nearby points, and it doesn’t assign weights or importance values to features inherently. Hence, KNN does not have a built-in way of determining feature importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP (does not work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/187 [08:29<1:45:14, 36.50s/it]"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize SHAP KernelExplainer with the KNN model and a sample of the training data\n",
    "explainer = shap.KernelExplainer(knn_model.predict, X_train.sample(100, random_state=42))\n",
    "\n",
    "# Increase the number of samples in X_test_sample to avoid the ValueError\n",
    "# The number of samples should be at least equal to the number of features to avoid convergence issues\n",
    "num_features = X_train.shape[1]\n",
    "X_test_sample = X_test.sample(max(num_features, 50), random_state=42)  # Ensure enough rows for stability\n",
    "\n",
    "# Calculate SHAP values for the same sample of the scaled test data\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Visualize SHAP values with a summary plot for the same test sample\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\") \n",
    "plt.title('SHAP Summary Plot for KNN Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/20284 [01:42<289:24:40, 51.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m X_test_sample \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmax\u001b[39m(num_features, \u001b[38;5;241m50\u001b[39m), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Ensure enough rows for stability\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the larger sample size\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Visualize SHAP values with a summary plot for the test sample\u001b[39;00m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\shap\\explainers\\_kernel.py:271\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    270\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 271\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    273\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\shap\\explainers\\_kernel.py:476\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[0;32m    479\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\shap\\explainers\\_kernel.py:615\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[0;32m    614\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m--> 615\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mf(data)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[0;32m    617\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:274\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    272\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X)\n\u001b[0;32m    276\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    277\u001b[0m _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:849\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    842\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    845\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    846\u001b[0m     )\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 849\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    850\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    851\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m    852\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[0;32m    853\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[0;32m    854\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[0;32m    855\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    856\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    857\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    861\u001b[0m ):\n\u001b[0;32m    862\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    863\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    864\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:278\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    279\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    280\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m    281\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    282\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m    283\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[0;32m    284\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[0;32m    285\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    286\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    291\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    292\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    299\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:59\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\threadpoolctl.py:592\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explainer = shap.KernelExplainer(knn_model.predict, X_train.sample(100, random_state=42))\n",
    "\n",
    "# Increase the number of samples in X_test_sample to avoid the ValueError\n",
    "# The number of samples should be at least equal to the number of features to avoid convergence issues\n",
    "num_features = X_train.shape[1]\n",
    "X_test_sample = X_test.sample(max(num_features, 50), random_state=42)  # Ensure enough rows for stability\n",
    "\n",
    "# Calculate SHAP values for the larger sample size\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize SHAP values with a summary plot for the test sample\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\") \n",
    "plt.title('SHAP Summary Plot for KNN Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a Lime Explainer object\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),  # Features from the training data\n",
    "    training_labels=np.array(y_train),  # Target labels for training data\n",
    "    mode=\"classification\",  # Set to 'classification' or 'regression' based on your task\n",
    "    feature_names=X_train.columns,  # Feature names\n",
    "    class_names=['Class 0', 'Class 1'],  # Class names for binary classification (modify if more classes)\n",
    "    discretize_continuous=True  # Discretizes continuous features for better explanation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label for selected instance: 0\n",
      "Instance features:\n",
      " c_0      0.977912\n",
      "c_1      0.791165\n",
      "c_2      0.020080\n",
      "c_3      0.022088\n",
      "c_4      0.002008\n",
      "           ...   \n",
      "c_182    0.000000\n",
      "c_183    0.000000\n",
      "c_184    0.000000\n",
      "c_185    0.000000\n",
      "c_186    0.000000\n",
      "Name: 25, Length: 187, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select a random instance from the test set\n",
    "idx = 25 # Change this index to select a different instance\n",
    "instance = X_test.iloc[idx]  # The input instance\n",
    "\n",
    "# Get the true label for the selected instance\n",
    "true_label = y_test.iloc[idx]\n",
    "\n",
    "print(\"True Label for selected instance:\", true_label)\n",
    "print(\"Instance features:\\n\", instance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
