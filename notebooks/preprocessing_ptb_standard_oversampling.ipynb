{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Default Preprocess For PTB data \n",
    "The default preprocessing step is what we conclude in our report. \n",
    "You can check below what should be input for this notebook and what would be the output if necessary.\n",
    "\n",
    "Processing **steps** for PTB dataset:   \n",
    "    resample: Oversampling  \n",
    "    rescaling: StandardScaler  \n",
    "\n",
    "\n",
    "**Input** : raw data:  \n",
    "ptbdb_normal.csv  \n",
    "ptbdb_abnormal.csv  \n",
    "\n",
    "**Output** : Sampled and Scaled data:   \n",
    "ptb_train_clean_standard_oversampling.csv  \n",
    "ptb_test_clean_standard_oversampling.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local environment\n",
      "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "data_path = ''\n",
    "data_output_path = ''\n",
    "# Check if the environment is Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    data_output_path = data_path\n",
    "    \n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/raw/'\n",
    "    data_output_path = '../data/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m_FSL5cYnEQI"
   },
   "outputs": [],
   "source": [
    "# Verify installation and import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'normal': data_path +  'ptbdb_normal.csv',\n",
    "    'abnormal': data_path +  'ptbdb_abnormal.csv',\n",
    "})\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'test': data_output_path +  'ptb_test_clean_standard_oversampling.csv',\n",
    "    'train': data_output_path +  'ptb_train_clean_standard_oversampling.csv',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hZhtCq2NnEQJ"
   },
   "outputs": [],
   "source": [
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_182'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "\n",
    "def convertColumnAsInt(df, column):\n",
    "    \"\"\"\n",
    "    As the category value is in float type. We want to get the int to identify the category.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ptb_normal = pd.read_csv(RawFiles.get('normal'), header=None)\n",
    "ptb_abnormal = pd.read_csv(RawFiles.get('abnormal'), header=None)\n",
    "\n",
    "ptb_data = pd.concat([ptb_normal, ptb_abnormal], axis=0, ignore_index=True)\n",
    "ptb_data = ptb_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "ptb_data = addColumnsToDataframe(ptb_data)  \n",
    "ptb_data = convertColumnAsInt(ptb_data, 'target') \n",
    "\n",
    "ptb_data = ptb_data.dropna(axis=0)\n",
    "\n",
    "\n",
    "y = ptb_data['target']\n",
    "X = ptb_data.drop(columns=['target'], inplace=False)\n",
    "\n",
    "# split train test set before resampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "randomOverSampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = randomOverSampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# rescaler with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# convert resampling rescaling data back to dataframe to concat\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f'c_{i}' for i in range(X_train_scaled.shape[1])])\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f'c_{i}' for i in range(X_test_scaled.shape[1])])\n",
    "y_train_resampled_df = pd.DataFrame(y_train_resampled, columns=['target'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['target'])\n",
    "\n",
    "# concat X_train, y_train(reset index to avoid join)\n",
    "ptb_train_clean_standard_oversampling = pd.concat(\n",
    "    [\n",
    "        X_train_scaled_df, y_train_resampled_df\n",
    "    ], axis=1)\n",
    "\n",
    "ptb_test_clean_standard_oversampling = pd.concat(\n",
    "    [  \n",
    "        X_test_scaled_df, y_test_df.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing is done and saved to the output folder\n",
      "Train data shape:  (16812, 188)\n",
      "Test data shape:  (2911, 188)\n",
      "Data saved to:  ../data/processed/ptb_train_clean_standard_oversampling.csv\n",
      "Data saved to:  ../data/processed/ptb_test_clean_standard_oversampling.csv\n"
     ]
    }
   ],
   "source": [
    "# save clean data to \n",
    "# Speichern der Trainings- und Testdaten mit Header\n",
    "ptb_train_clean_standard_oversampling.to_csv(OutputFiles.get('train'), index=False, header=True)\n",
    "ptb_test_clean_standard_oversampling.to_csv(OutputFiles.get('test'), index=False, header=True)\n",
    "\n",
    "# show some info\n",
    "print(\"Data Preprocessing is done and saved to the output folder\")\n",
    "print(\"Train data shape: \", ptb_train_clean_standard_oversampling.shape)\n",
    "print(\"Test data shape: \", ptb_test_clean_standard_oversampling.shape)\n",
    "print(\"Data saved to: \", OutputFiles.get('train'))\n",
    "print(\"Data saved to: \", OutputFiles.get('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2024-11-13 10:05:21.141005\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
