{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Default Preprocess For MIT data \n",
        "The default preprocessing step is what we conclude in our report. \n",
        "You can check below what should be input for this notebook and what would be the output if necessary.\n",
        "\n",
        "Processing **steps** for MIT dataset:   \n",
        "    resample: SMOTE   \n",
        "    rescaling: StandardScaler  \n",
        "\n",
        "\n",
        "**Input** : the original data.   \n",
        "mitbih_test.csv\n",
        "mitbih_train.csv\n",
        "\n",
        "**Output** : The cleaning data.   \n",
        "mitbih_train_clean_default.csv\n",
        "mitbih_test_clean_default.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local environment\n",
            "Current working directory: /Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "data_path = ''\n",
        "data_output_path = ''\n",
        "# Check if the environment is Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running on Google Colab\")\n",
        "    # Install required libraries\n",
        "    !pip install scikit-learn -q\n",
        "    !pip install pandas -q\n",
        "    !pip install numpy -q\n",
        "    !pip install imbalanced-learn -q\n",
        "\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # set the path where the csv file stored in your google drive. \n",
        "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
        "    data_output_path = data_path\n",
        "    \n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "\n",
        "    current_path = os.getcwd()\n",
        "    print(\"Current working directory:\", current_path)\n",
        "    data_path = '../data/raw/'\n",
        "    data_output_path = '../data/processed/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_FSL5cYnEQI"
      },
      "outputs": [],
      "source": [
        "# Verify installation and import libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "RawFiles = dict({\n",
        "    'test': data_path +  'mitbih_test.csv',\n",
        "    'train': data_path +  'mitbih_train.csv',\n",
        "})\n",
        "\n",
        "OutputFiles = dict({\n",
        "    'test': data_output_path +  'mitbih_test_clean_default.csv',\n",
        "    'train': data_output_path +  'mitbih_train_clean_default.csv',\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hZhtCq2NnEQJ"
      },
      "outputs": [],
      "source": [
        "def addColumnsToDataframe(df):\n",
        "    \"\"\"\n",
        "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
        "    so we give the last column the name 'target', others named with 'c_182'\n",
        "    \"\"\"\n",
        "    num_columns= df.shape[1]\n",
        "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
        "    df_columns = feature_col_name + ['target']\n",
        "    df.columns = df_columns\n",
        "    return df\n",
        "\n",
        "def convertColumnAsInt(df, column):\n",
        "    \"\"\"\n",
        "    As the category value is in float type. We want to get the int to identify the category.\n",
        "    \"\"\"\n",
        "    df[column] = df[column].astype(int)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mitbih_train = pd.read_csv(RawFiles.get('train'), header=None ) \n",
        "mitbih_test = pd.read_csv(RawFiles.get('test'), header=None )\n",
        "\n",
        "mitbih_train = addColumnsToDataframe(mitbih_train)\n",
        "mitbih_train = convertColumnAsInt(mitbih_train, 'target')\n",
        "\n",
        "mitbih_test = addColumnsToDataframe(mitbih_test)\n",
        "mitbih_test = convertColumnAsInt(mitbih_test, 'target')\n",
        "\n",
        "# target value and meanings\n",
        "class_mapping = {\n",
        "    0: 'Normal',\n",
        "    1: 'Supraventricular',\n",
        "    2: 'Ventricular',\n",
        "    3: 'Fusion',\n",
        "    4: 'Unclassifiable'\n",
        "}\n",
        "\n",
        "#drop null value  \n",
        "mitbih_train = mitbih_train.dropna(how='any')\n",
        "mitbih_test = mitbih_test.dropna(how='any')\n",
        "\n",
        "#split train test set before resample\n",
        "y_train = mitbih_train['target']\n",
        "X_train = mitbih_train.drop(columns=['target'], inplace=False)\n",
        "y_test = mitbih_test['target']\n",
        "X_test = mitbih_test.drop(columns=['target'], inplace=False)\n",
        "\n",
        "#resampling with SMOTE before rescaler\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#rescaler with StandardScaler \n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert resampling rescaling data back to dataframe to concat\n",
        "X_train_scaled_df =  pd.DataFrame(X_train_scaled, columns=[f'c_{i}' for i in range(X_train_scaled.shape[1])])\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f'c_{i}' for i in range(X_test_scaled.shape[1])])\n",
        "y_train_resampled_df = pd.DataFrame(y_train_resampled, columns=['target'])\n",
        "\n",
        "# concat X_train, y_train/ X_test, y_test\n",
        "mitbih_train_clean_default = pd.concat(\n",
        "    [\n",
        "        X_train_scaled_df,\n",
        "        y_train_resampled_df\n",
        "    ], axis=1)\n",
        "\n",
        "mitbih_test_clean_default = pd.concat(\n",
        "    [  \n",
        "        X_test_scaled_df,\n",
        "        y_test.reset_index(drop=True)\n",
        "    ], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save clean data to \n",
        "# mitbih_train_clean_default.csv\n",
        "# mitbih_test_clean_default.csv\n",
        "\n",
        "mitbih_train_clean_default.to_csv(OutputFiles.get('train'), index=False)\n",
        "mitbih_test_clean_default.to_csv(OutputFiles.get('test'), index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
