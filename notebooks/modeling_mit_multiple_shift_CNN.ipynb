{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzMa6vRFMqhY"
   },
   "source": [
    "# CNN Model (multiple) for shifted MIT data\n",
    "As we concluded before, for MIT data, we apply the following preprocessing:   \n",
    "resampling: Oversampling \\\n",
    "rescaling: MinMax Scaler\n",
    "\n",
    "If you don't have the original files: run the notebook 'preprocessing_mit_multipleclass_shift_minmax_oversampling.ipynb'    \n",
    "Input file: (The preprocessed data)   \n",
    "mitbih_train_multiclass_shift_minmax_oversampling.csv\n",
    "mitbih_test_multiclass_shift_minmax_oversampling.csv\n",
    "\n",
    "Output: CNN model trained  \n",
    "model_mit_multiple_shift_cnn.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqzSKSCzMqha",
    "outputId": "578ce4f4-246a-4792-e129-00e57545c9e0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "    !pip install matplotlib -q\n",
    "    !pip install seaborn -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive.\n",
    "    data_path = '/content/drive/MyDrive/data/'\n",
    "    model_output_path = data_path\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waufOFi2Mqha"
   },
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPPl-CRsMqhb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use ImbPipeline for oversampling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'mitbih_multipleclass_train_shift_minmax_oversampling.csv',\n",
    "    'test': data_path + 'mitbih_multipleclass_test_shift_minmax_oversampling.csv'\n",
    "})\n",
    "\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'model_mit_multiple_shift_cnn' \n",
    "})\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3xxybnyMqhb",
    "outputId": "5522fd1c-5e21-4b18-d2c8-db10788c0657"
   },
   "outputs": [],
   "source": [
    "# Convert DataFrames to NumPy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Reshape data for (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Check the shape\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, timesteps, 1)\n",
    "print(\"X_test shape:\", X_test.shape)    # Should be (num_samples, timesteps, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=4)\n",
    "y_test = to_categorical(y_test, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMHx7qNhMqhb"
   },
   "source": [
    "# 1D CNN with MinMax Scaler and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ww_pffBnMqhb",
    "outputId": "ca8215a1-b2be-417e-b66d-7da576e1c797"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Define the 1D CNN model for multi-class classification\n",
    "model = Sequential()\n",
    "model.add(Conv1D(187, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    factor=0.1,\n",
    "    cooldown=4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_learning_rate],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Scores\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['sparse_categorical_accuracy']\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "print(f\"Final Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}, \"\n",
    "      f\"Accuracy: {train_accuracy[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get class predictions\n",
    "print(f\"Predicted Classes: {y_pred_classes[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the Loss\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Model Loss by Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Accuracy\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy by Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to class predictions\n",
    "y_pred_class = np.argmax(y_pred, axis=1) # argmax returns the index of the max value\n",
    "\n",
    "# Convert one-hot encoded `y_test` to class labels\n",
    "y_test_class = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsacJsuZMqhb",
    "outputId": "b04ee3ef-eec8-4e7e-b822-6a5836e6d6e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_class, y_pred_class))\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "# F1 Score\n",
    "print(f\"F1 Score: {f1_score(y_test_class, y_pred_class, average='weighted'):.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzQ_K5ngMqhc"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbRL8H1_Mqhc",
    "outputId": "157083c0-c43b-4f34-af11-106534002675"
   },
   "outputs": [],
   "source": [
    "# Save the model in HDF5 format\n",
    "model_path = OutputFiles.get('model') + '.h5'  # Append .h5 extension\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJKRXO6FMqhc"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
