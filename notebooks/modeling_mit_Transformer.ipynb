{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model for MIT data \n",
    "As we concluded before, for MIT data, we apply the following preprocessing:   \n",
    "resampling: Oversampling  \n",
    "rescaling: MixMax Scaler\n",
    "\n",
    "If you don't have the original files: run the notebook `preprocessing_mit_minmax_oversampling.ipynb`     \n",
    "Input file:(The preprocessed data)     \n",
    "mit_train_clean_mixmax_oversampling.csv \n",
    "mit_test_clean_mixmax_oversampling.csv \n",
    "\n",
    "Output: Transformer model trained  \n",
    "model_mit_trans.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab \n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install imbalanced-learn -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    model_output_path = data_path\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use ImbPipeline for oversampling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'mitbih_train_clean_minmax_oversampling.csv', \n",
    "    'test': data_path + 'mitbih_test_clean_minmax_oversampling.csv'  \n",
    "})\n",
    "\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'model_mit_trans' \n",
    "})\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with MinMax Scaler and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to NumPy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Reshape the data to fit the LSTM model (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Check the shape of the data\n",
    "print(\"X_train shape:\", X_train.shape)  # samples, timesteps, features\n",
    "print(\"X_test shape:\", X_test.shape)    # samples, timesteps, features\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dropout, Add, LayerNormalization, Dense, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Optimized Transformer Encoder Layer\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.4, l2_reg=1e-5):  # Increased dropout to 0.4\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Add()([x, inputs])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Feed-Forward Network\n",
    "    x_ff = Dense(ff_dim, activation=\"relu\", kernel_regularizer=l2(l2_reg))(x)\n",
    "    x_ff = Dropout(dropout)(x_ff)\n",
    "    x_ff = Dense(inputs.shape[-1], kernel_regularizer=l2(l2_reg))(x_ff)\n",
    "    x = Add()([x, x_ff])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# Stacked Transformer Encoder Layers\n",
    "for _ in range(4):  # Stack the encoder block 4 times\n",
    "    x = transformer_encoder(inputs if _ == 0 else x, head_size=64, num_heads=8, ff_dim=256, dropout=0.2, l2_reg=1e-5)\n",
    "\n",
    "# Global Pooling and Dense Layers for Classification\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.5)(x) \n",
    "x = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Define Model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(learning_rate=0.0001)  # Reduced learning rate for stable training\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test), \n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Testing/Evaluation on Test Set\n",
    "test_loss, test_accuracy, test_auc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test AUC: {test_auc}\")\n",
    "\n",
    "# Generate Predictions and Classification Report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training and validation loss and accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the Loss\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Model Loss by Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Accuracy\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy by Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
