{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Default Preprocess For PTB data \n",
        "The default preprocessing step is what we conclude in our report. \n",
        "You can check below what should be input for this notebook and what would be the output if necessary.\n",
        "\n",
        "Processing **steps** for PTB dataset:   \n",
        "    resample: Oversampling  \n",
        "    rescaling: StandardScaler  \n",
        "\n",
        "\n",
        "**Input** : raw data:\n",
        "ptbdb_normal.csv\n",
        "ptbdb_abnormal.csv\n",
        "\n",
        "**Output** : Sampled and Scaled data:   \n",
        "ptb_train_clean_standard_oversampling.csv  \n",
        "ptb_test_clean_standard_oversampling.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local environment\n",
            "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "data_path = ''\n",
        "data_output_path = ''\n",
        "# Check if the environment is Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running on Google Colab\")\n",
        "    # Install required libraries\n",
        "    !pip install scikit-learn -q\n",
        "    !pip install pandas -q\n",
        "    !pip install numpy -q\n",
        "    !pip install imbalanced-learn -q\n",
        "\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # set the path where the csv file stored in your google drive. \n",
        "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
        "    data_output_path = data_path\n",
        "    \n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "\n",
        "    current_path = os.getcwd()\n",
        "    print(\"Current working directory:\", current_path)\n",
        "    data_path = '../data/raw/'\n",
        "    data_output_path = '../data/processed/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m_FSL5cYnEQI"
      },
      "outputs": [],
      "source": [
        "# Verify installation and import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "RawFiles = dict({\n",
        "    'normal': data_path +  'ptbdb_normal.csv',\n",
        "    'abnormal': data_path +  'ptbdb_abnormal.csv',\n",
        "})\n",
        "\n",
        "OutputFiles = dict({\n",
        "    'test': data_output_path +  'ptb_test_clean_standard_oversamling.csv',\n",
        "    'train': data_output_path +  'ptb_train_clean_standard_oversampling.csv',\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hZhtCq2NnEQJ"
      },
      "outputs": [],
      "source": [
        "def addColumnsToDataframe(df):\n",
        "    \"\"\"\n",
        "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
        "    so we give the last column the name 'target', others named with 'c_182'\n",
        "    \"\"\"\n",
        "    num_columns= df.shape[1]\n",
        "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
        "    df_columns = feature_col_name + ['target']\n",
        "    df.columns = df_columns\n",
        "    return df\n",
        "\n",
        "def convertColumnAsInt(df, column):\n",
        "    \"\"\"\n",
        "    As the category value is in float type. We want to get the int to identify the category.\n",
        "    \"\"\"\n",
        "    df[column] = df[column].astype(int)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Preprocessing is done and saved to the output folder\n",
            "Train data shape:  (16809, 188)\n",
            "Test data shape:  (4203, 188)\n",
            "Data saved to:  ../data/processed/ptb_train_clean_standard_oversampling.csv\n",
            "Data saved to:  ../data/processed/ptb_test_clean_standard_oversamling.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Lesen der Dateien ein\n",
        "ptb_normal = pd.read_csv(RawFiles.get('normal'), header=None)\n",
        "ptb_abnormal = pd.read_csv(RawFiles.get('abnormal'), header=None)\n",
        "\n",
        "# Kombinieren der beiden DataFrames\n",
        "ptb_data = pd.concat([ptb_normal, ptb_abnormal], axis=0, ignore_index=True)\n",
        "\n",
        "# Annahme: Die Funktionen `addColumnsToDataframe` und `convertColumnAsInt` sind vorhanden\n",
        "ptb_data = addColumnsToDataframe(ptb_data)\n",
        "ptb_data = convertColumnAsInt(ptb_data, 'target')\n",
        "\n",
        "# Entfernen von fehlenden Werten\n",
        "ptb_data = ptb_data.dropna(axis=0)\n",
        "\n",
        "# Oversampling anwenden, um die Klassen auszugleichen\n",
        "X = ptb_data.drop(columns=['target'])\n",
        "y = ptb_data['target']\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Zusammenführen der resampled Daten in einen DataFrame\n",
        "ptb_data = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# Skalieren der Daten mit StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X = ptb_data.drop(columns=['target'])\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y = ptb_data['target']\n",
        "\n",
        "# Splitten der Daten in Trainings- und Test-Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Spaltenüberschriften definieren\n",
        "columns = [f'c_{i}' for i in range(X.shape[1])] + ['target']\n",
        "\n",
        "# Umwandlung der skalierten Daten in Pandas DataFrames mit Spaltenüberschriften\n",
        "X_train_scaled_df = pd.DataFrame(X_train, columns=columns[:-1])\n",
        "X_test_scaled_df = pd.DataFrame(X_test, columns=columns[:-1])\n",
        "\n",
        "# Zielspalten in DataFrames umwandeln und den Namen \"target\" zuweisen\n",
        "y_train = pd.DataFrame(y_train, columns=['target']).reset_index(drop=True)\n",
        "y_test = pd.DataFrame(y_test, columns=['target']).reset_index(drop=True)\n",
        "\n",
        "# Zusammenführen von X_train_scaled und y_train sowie X_test_scaled und y_test\n",
        "ptb_train_clean_default = pd.concat([X_train_scaled_df, y_train], axis=1)\n",
        "ptb_test_clean_default = pd.concat([X_test_scaled_df, y_test], axis=1)\n",
        "\n",
        "# Sicherstellen, dass die Spaltenüberschriften korrekt sind\n",
        "assert list(ptb_train_clean_default.columns) == columns, \"Trainingsdaten haben nicht die erwarteten Überschriften!\"\n",
        "assert list(ptb_test_clean_default.columns) == columns, \"Testdaten haben nicht die erwarteten Überschriften!\"\n",
        "\n",
        "# Speichern der Trainings- und Testdaten mit Header\n",
        "# Trainingsdaten mit Header speichern\n",
        "ptb_train_clean_default.to_csv(OutputFiles.get('train'), index=False, header=True)\n",
        "\n",
        "# Testdaten mit Header speichern\n",
        "ptb_test_clean_default.to_csv(OutputFiles.get('test'), index=False, header=True)\n",
        "\n",
        "# Statusmeldung\n",
        "print(\"Data Preprocessing is done and saved to the output folder\")\n",
        "print(\"Train data shape: \", ptb_train_clean_default.shape)\n",
        "print(\"Test data shape: \", ptb_test_clean_default.shape)\n",
        "print(\"Data saved to: \", OutputFiles.get('train'))\n",
        "print(\"Data saved to: \", OutputFiles.get('test'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train_scaled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# as after scaler, the X_train_scaled X_test_scaled is numpy.ndarray\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# convert numpy.ndarray to pandas again with header before concat X_train y_train\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train_scaled_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_scaled, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[0;32m      5\u001b[0m X_test_scaled_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_scaled, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# concat X_train, y_train(reset index to avoid join)\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
          ]
        }
      ],
      "source": [
        "# as after scaler, the X_train_scaled X_test_scaled is numpy.ndarray\n",
        "# convert numpy.ndarray to pandas again with header before concat X_train y_train\n",
        "\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f'c_{i}' for i in range(X.shape[1])])\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=[f'c_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "\n",
        "# concat X_train, y_train(reset index to avoid join)\n",
        "ptb_train_clean_default = pd.concat(\n",
        "    [\n",
        "        X_train_scaled_df, y_train.reset_index(drop=True) \n",
        "    ], axis=1)\n",
        "\n",
        "ptb_test_clean_default = pd.concat(\n",
        "    [  \n",
        "        X_test_scaled_df, y_test.reset_index(drop=True)\n",
        "    ], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11641"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ptb_train_clean_default.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save clean data to \n",
        "# ptb_train_clean_default.csv\n",
        "# ptb_test_clean_default.csv\n",
        "ptb_train_clean_default.to_csv(OutputFiles.get('train'), index=False)\n",
        "ptb_test_clean_default.to_csv(OutputFiles.get('test'), index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
