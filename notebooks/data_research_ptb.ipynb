{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Check enviroment**\n",
    "\n",
    "Set data_path to make notebooks works well both on local and google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "data_path = ''\n",
    "# Check if the environment is Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install tensorflow -q\n",
    "    !pip install keras -q\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install matplotlib -q\n",
    "    !pip install umap-learn\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your own google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    \n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/raw/'\n",
    "\n",
    "Path = dict({\n",
    "    'ptbdb_normal': data_path +  'ptbdb_normal.csv',\n",
    "    'ptbdb_abnormal':  data_path + 'ptbdb_abnormal.csv',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify installation and import libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_182'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "\n",
    "def convertColumnAsInt(df, column):\n",
    "    \"\"\"\n",
    "    As the category value is in float type. We want to get the int to identify the category.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n",
    "def getBarChartFromCategoryValueCounts(category_value_counts):\n",
    "    \"\"\"\n",
    "    We call the plot over the pandas series object to plot the category count values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar_chart = category_value_counts.plot(kind='bar')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=360)\n",
    "    for i in bar_chart.containers:\n",
    "        bar_chart.bar_label(i, label_type='edge')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def showTop10DataInChart(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xDataAxis = list(range(0, df.shape[1]))\n",
    "    yDataRows = list(df.values[1: 10])\n",
    "    for y in yDataRows:\n",
    "        plt.plot(xDataAxis, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_normal = pd.read_csv(Path.get('ptbdb_normal'), header=None ) \n",
    "ptbdb_normal_with_columns = addColumnsToDataframe(ptbdb_normal) # add columns to the dataframe\n",
    "ptbdb_normal_with_columns = convertColumnAsInt(ptbdb_normal_with_columns, 'target') # convert the target column to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_normal_with_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_abnormal = pd.read_csv(Path.get('ptbdb_abnormal'), header=None ) \n",
    "ptbdb_abnormal_with_columns = addColumnsToDataframe(ptbdb_abnormal) # add columns to the dataframe\n",
    "ptbdb_abnormal_with_columns = convertColumnAsInt(ptbdb_abnormal_with_columns, 'target') # convert the target column to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_abnormal_with_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge normal and abnomal data to one dataset and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the datasets\n",
    "ptbdb = pd.concat([ptbdb_abnormal_with_columns, ptbdb_normal_with_columns], ignore_index=True) # ingore the index to make the index continuous\n",
    "#Shuffle the dataset\n",
    "ptbdb = ptbdb.sample(frac=1).reset_index(drop=True)\n",
    "ptbdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training (80%) and testing (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "ptbdb_train, ptbdb_test = train_test_split(ptbdb, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train:\n",
    "print(ptbdb_train.shape)\n",
    "print(\"The train data has 11.641 and 188 columns.\")\n",
    "\n",
    "#test:\n",
    "print(ptbdb_test.shape)\n",
    "print(\"The test data has 2.911 and 188 columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptbdb_train[\"target\"])\n",
    "print(ptbdb_test[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptbdb_train.dtypes)\n",
    "print(ptbdb_test.dtypes)\n",
    "print(\"The features are numeric. All columns have the type float64, while the target column is int32.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptbdb_test.info(show_counts=True), end=\"\\n\\n\")\n",
    "print(\"Size of the DataFrame\", ptbdb_test.shape, end='\\n\\n')\n",
    "\n",
    "print(ptbdb_train.info(show_counts=True), end=\"\\n\\n\")\n",
    "print(\"Size of the DataFrame\", ptbdb_train.shape, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rows_duplicated = ptbdb_train.duplicated().sum()\n",
    "print(\"Number of rows duplicated :\", nb_rows_duplicated)\n",
    "\n",
    "nb_rows_duplicated = ptbdb_test.duplicated().sum()\n",
    "print(\"Number of rows duplicated :\", nb_rows_duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the training set\n",
    "ptbdb_train = ptbdb_train.drop_duplicates()\n",
    "\n",
    "# Remove duplicates from the testing set\n",
    "ptbdb_test = ptbdb_test.drop_duplicates()\n",
    "\n",
    "# Verify the removal by checking the number of duplicated rows again\n",
    "nb_rows_duplicated_train = ptbdb_train.duplicated().sum()\n",
    "print(\"Number of rows duplicated in train set after removal:\", nb_rows_duplicated_train)\n",
    "\n",
    "nb_rows_duplicated_test = ptbdb_test.duplicated().sum()\n",
    "print(\"Number of rows duplicated in test set after removal:\", nb_rows_duplicated_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in train:\", ptbdb_train.isnull().sum())\n",
    "print(\"Missing values in test:\", ptbdb_test.isnull().sum())\n",
    "print(\"The data has no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping dictionary\n",
    "class_mapping = {\n",
    "    1: 'Normal',\n",
    "    0: 'Abnormal'\n",
    "}\n",
    "\n",
    "# Define custom colors for each category\n",
    "color_mapping = {\n",
    "    0: 'green',    # Normal beat\n",
    "    1: 'red',      # Abnormal beat \n",
    "}\n",
    "\n",
    "classes_to_plot = [0, 1]\n",
    "\n",
    "# Calculate value counts based on mapped class names\n",
    "value_counts_series_train = ptbdb_train['target'].map(class_mapping).value_counts()\n",
    "# Calculate value counts based on mapped class names\n",
    "value_counts_series_test = ptbdb_test['target'].map(class_mapping).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Dataset Class Distribution:\")\n",
    "print(value_counts_series_train)\n",
    "\n",
    "print(\"\\nTesting Dataset Class Distribution:\")\n",
    "print(value_counts_series_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Set the figure size\n",
    "bar_chart = value_counts_series_train.plot(kind='bar', color=color_mapping.values()) # Create a bar chart for the value counts of the target column in the training dataset\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Target Categories (Training Dataset)')\n",
    "plt.xticks(rotation=45, ha='right') # Rotate the x-axis labels\n",
    "\n",
    "# Add labels to the bars\n",
    "for container in bar_chart.containers: # Iterate over the bar containers\n",
    "    plt.bar_label(container, label_type='edge') # Add labels to the bars\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# test data \n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_chart = value_counts_series_test.plot(kind='bar', color=color_mapping.values())\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Target Categories (Testing Dataset)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels to the bars\n",
    "for container in bar_chart.containers:\n",
    "    plt.bar_label(container, label_type='edge')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of each class once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot overlay of ECG signals from both datasets for a single class\n",
    "def plot_overlay_ecg_signals(df, label, color, dataset_label): # Define a function to plot overlay of ECG signals for a single class from both datasets\n",
    "    \"\"\"\n",
    "    Plot overlay of ECG signals for a single class from both datasets.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing ECG signals and target labels\n",
    "    label (str or int): Class label to plot\n",
    "    color (str): Color for the plot\n",
    "    dataset_label (str): Label for the dataset (e.g., 'Training', 'Testing')\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the descriptive label to the corresponding class label\n",
    "    if isinstance(label, str):\n",
    "        class_label = [k for k, v in class_mapping.items() if v == label][0] # Get the class label for the specified class name\n",
    "    else:\n",
    "        class_label = label # Use the specified class label\n",
    "    \n",
    "    # Extract data rows for the specified class label\n",
    "    class_data = df[df['target'] == class_label]\n",
    "\n",
    "    # Check if there is any data for the specified class label\n",
    "    if class_data.empty:\n",
    "        print(f\"No data found for class {class_mapping[class_label]}\")\n",
    "        return\n",
    "    \n",
    "    # Extract a sample data row (first row) for the specified class label\n",
    "    sample_data = class_data.iloc[0]\n",
    "\n",
    "    # Plot the sample ECG signal, excluding the 'target' column\n",
    "    plt.plot(sample_data[:-1], label=f'{dataset_label}: {class_mapping[class_label]}', color=color)\n",
    "\n",
    "    plt.title(f\"Overlay of ECG Signals - {dataset_label}\")\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.xticks([])  # Remove x-axis ticks and labels\n",
    "    plt.legend()\n",
    "\n",
    "# List of classes to plot\n",
    "classes_to_plot = ['Normal', 'Abnormal']\n",
    "\n",
    "# Colors for each class\n",
    "colors = ['green','red']\n",
    "\n",
    "# Plotting overlay for each class in both datasets\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting for training dataset\n",
    "for label, color in zip(classes_to_plot, colors):\n",
    "    plot_overlay_ecg_signals(ptbdb_train, label, color, 'Training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting for testing dataset\n",
    "for label, color in zip(classes_to_plot, colors):\n",
    "    plot_overlay_ecg_signals(ptbdb_test, label, color, 'Testing')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
