{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global and Local Explanation for the Decision Tree and XGBoost model on PTB data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local environment\n",
      "Current working directory: g:\\Meine Ablage\\heartbeat-analysis-ai\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "data_path = ''\n",
    "# Check if the environment is Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    !pip install tensorflow -q\n",
    "    !pip install keras -q\n",
    "    !pip install scikit-learn -q\n",
    "    !pip install pandas -q\n",
    "    !pip install numpy -q\n",
    "    !pip install matplotlib -q\n",
    "    !pip install umap-learn -q\n",
    "    !pip install lightgbm\n",
    "    !pip install xgboost\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your own google drive. \n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
    "    \n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/raw/'\n",
    "\n",
    "Path = dict({\n",
    "    'ptbdb_normal': data_path +  'ptbdb_normal.csv',\n",
    "    'ptbdb_abnormal':  data_path + 'ptbdb_abnormal.csv',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import umap\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnsToDataframe(df):\n",
    "    \"\"\"\n",
    "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
    "    so we give the last column the name 'target', others named with 'c_182'\n",
    "    \"\"\"\n",
    "    num_columns= df.shape[1]\n",
    "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
    "    df_columns = feature_col_name + ['target']\n",
    "    df.columns = df_columns\n",
    "    return df\n",
    "\n",
    "def convertColumnAsInt(df, column):\n",
    "    \"\"\"\n",
    "    As the category value is in float type. We want to get the int to identify the category.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n",
    "def getBarChartFromCategoryValueCounts(category_value_counts):\n",
    "    \"\"\"\n",
    "    We call the plot over the pandas series object to plot the category count values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar_chart = category_value_counts.plot(kind='bar')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=360)\n",
    "    for i in bar_chart.containers:\n",
    "        bar_chart.bar_label(i, label_type='edge')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def showTop10DataInChart(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xDataAxis = list(range(0, df.shape[1]))\n",
    "    yDataRows = list(df.values[1: 10])\n",
    "    for y in yDataRows:\n",
    "        plt.plot(xDataAxis, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_normal = pd.read_csv(Path.get('ptbdb_normal'), header=None ) \n",
    "ptbdb_normal_with_columns = addColumnsToDataframe(ptbdb_normal) # add columns to the dataframe\n",
    "ptbdb_normal_with_columns = convertColumnAsInt(ptbdb_normal_with_columns, 'target') # convert the target column to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_abnormal = pd.read_csv(Path.get('ptbdb_abnormal'), header=None ) \n",
    "ptbdb_abnormal_with_columns = addColumnsToDataframe(ptbdb_abnormal) # add columns to the dataframe\n",
    "ptbdb_abnormal_with_columns = convertColumnAsInt(ptbdb_abnormal_with_columns, 'target') # convert the target column to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the datasets\n",
    "ptbdb = pd.concat([ptbdb_abnormal_with_columns, ptbdb_normal_with_columns], ignore_index=True) # ingore the index to make the index continuous\n",
    "#Shuffle the dataset\n",
    "ptbdb = ptbdb.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training (80%) and testing (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "ptbdb_train, ptbdb_test = train_test_split(ptbdb, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the training set\n",
    "ptbdb_train = ptbdb_train.drop_duplicates()\n",
    "\n",
    "# Remove duplicates from the testing set\n",
    "ptbdb_test = ptbdb_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X_train = ptbdb_train.drop(columns=['target'])\n",
    "y_train = ptbdb_train['target']\n",
    "\n",
    "X_test = ptbdb_test.drop(columns=['target'])\n",
    "y_test = ptbdb_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the Pickle file (XGBoost model) using a raw string to handle backslashes\n",
    "model_path = r'G:\\Meine Ablage\\heartbeat-analysis-ai\\models\\model_ptb_XGBoost.pkl'  \n",
    "with open(model_path, 'rb') as file:\n",
    "    xgb_model = pickle.load(file)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded model is not a valid XGBModel or Booster.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the loaded model is an instance of XGBModel or Booster\n",
    "from xgboost import XGBModel, Booster\n",
    "\n",
    "if isinstance(xgb_model, (XGBModel, Booster)):\n",
    "    print(\"Model loaded as XGBModel or Booster.\")\n",
    "else:\n",
    "    print(\"The loaded model is not a valid XGBModel or Booster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tree must be Booster, XGBModel or dict instance",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot feature importance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m plot_importance(xgb_model)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost Feature Importance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Isabell Gurstein\\anaconda3\\envs\\project_env\\Lib\\site-packages\\xgboost\\plotting.py:96\u001b[0m, in \u001b[0;36mplot_importance\u001b[1;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, fmap, importance_type, max_num_features, grid, show_values, values_format, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     importance \u001b[38;5;241m=\u001b[39m booster\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree must be Booster, XGBModel or dict instance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m importance:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBooster.get_score() results in empty.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis maybe caused by having all trees as decision dumps.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: tree must be Booster, XGBModel or dict instance"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(xgb_model)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to explain the model predictions\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X)  # Using the full dataset X\n",
    "\n",
    "# Plot SHAP summary plot (explains the contribution of each feature)\n",
    "shap.summary_plot(shap_values, X)\n",
    "\n",
    "# Plot SHAP force plot for an individual prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explanation\n",
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X.values, \n",
    "    training_labels=y.values, \n",
    "    mode='classification',\n",
    "    feature_names=X.columns\n",
    ")\n",
    "\n",
    "# Choose a test instance to explain (e.g., the first instance in the dataset)\n",
    "idx = 0  # You can choose any row from the combined dataset to explain\n",
    "exp = explainer_lime.explain_instance(X.values[idx], xgb_model.predict_proba)\n",
    "\n",
    "# Show the LIME explanation\n",
    "exp.show_in_notebook(show_table=True, show_all=False)\n",
    "\n",
    "# You can also plot the LIME explanation as a bar chart\n",
    "exp.as_pyplot_figure()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
