{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwz-SOX1wMbZ"
   },
   "source": [
    "### Baseline Models and optimization steps for raw MIT data\n",
    " (Logistic Regression, Decision Tree, Naive Bayes, Random Forest)  \n",
    "\n",
    "as a seperate file we have applied KNN model on MIT data (run the notebook: modeling_mit_knn.ipynb)   \n",
    "\n",
    "If you don't have the original files: run the notebook   \n",
    "`preprocessing_mit_clean.ipynb`   \n",
    "\n",
    "Input file: raw data  \n",
    "mitbih_train_clean.csv   \n",
    "mitbih_test_clean.csv   \n",
    "\n",
    "Output:\n",
    "accuracy and classification reports of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjcKZtbJpLIo",
    "outputId": "c069c285-37e0-4023-f6a2-e9e57c9fa9e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local environment\n",
      "Current working directory: /Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "data_path = ''\n",
    "model_output_path = ''\n",
    "# check if the enviorment is Google Colab\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Install required libraries\n",
    "    # !pip install scikit-learn -q\n",
    "    # !pip install pandas -q\n",
    "    # !pip install numpy -q\n",
    "    # !pip install imbalanced-learn -q\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # set the path where the csv file stored in your google drive.\n",
    "    data_path = '/content/drive/MyDrive/Heartbeat_Project_me/preprocessed_data/'\n",
    "    model_output_path = '/content/drive/MyDrive/Heartbeat_Project_me/model_output/'\n",
    "\n",
    "else:\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    print(\"Current working directory:\", current_path)\n",
    "    data_path = '../data/processed/'\n",
    "    model_output_path = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxLUweCPpEAZ",
    "outputId": "276167f7-44bd-406c-925b-af6ae9856029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9114080063103924\n",
      "Logistic Regression Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     18118\n",
      "           1       0.73      0.27      0.40      2166\n",
      "\n",
      "    accuracy                           0.91     20284\n",
      "   macro avg       0.82      0.63      0.67     20284\n",
      "weighted avg       0.90      0.91      0.89     20284\n",
      "\n",
      "Decision Tree Accuracy: 0.9638138434233879\n",
      "Decision Tree Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     18118\n",
      "           1       0.84      0.82      0.83      2166\n",
      "\n",
      "    accuracy                           0.96     20284\n",
      "   macro avg       0.91      0.90      0.90     20284\n",
      "weighted avg       0.96      0.96      0.96     20284\n",
      "\n",
      "Naive Bayes Accuracy: 0.8454939854072175\n",
      "Naive Bayes Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     18118\n",
      "           1       0.31      0.36      0.33      2166\n",
      "\n",
      "    accuracy                           0.85     20284\n",
      "   macro avg       0.62      0.63      0.62     20284\n",
      "weighted avg       0.86      0.85      0.85     20284\n",
      "\n",
      "Random Forest Accuracy: 0.9794912246105305\n",
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     18118\n",
      "           1       0.99      0.82      0.89      2166\n",
      "\n",
      "    accuracy                           0.98     20284\n",
      "   macro avg       0.98      0.91      0.94     20284\n",
      "weighted avg       0.98      0.98      0.98     20284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "RawFiles = dict({\n",
    "    'train': data_path + 'mitbih_train_clean.csv',\n",
    "    'test': data_path + 'mitbih_test_clean.csv'\n",
    "})\n",
    "\n",
    "\n",
    "OutputFiles = dict({\n",
    "    'model': model_output_path +  'baseline_models_mit_raw.csv',\n",
    "    'Optimization' : model_output_path + 'optimization_baseline_models_mit_raw.csv'\n",
    "})\n",
    "\n",
    "\n",
    "train = pd.read_csv(RawFiles.get('train'),sep=',',header=0)\n",
    "test = pd.read_csv(RawFiles.get('test'),sep=',',header=0)\n",
    "\n",
    "y_train = train['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "\n",
    "y_test = test['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "\n",
    "# Baseline model 1: Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Baseline model 2: Decision Tree\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(X_train, y_train)\n",
    "y_pred_dec_tree = dec_tree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dec_tree))\n",
    "print(\"Decision Tree Report:\\n\", classification_report(y_test, y_pred_dec_tree))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Baseline model 4: Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Baseline model 5: Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8z0BP9ugphem",
    "outputId": "5fac9820-2d0f-451f-8282-93b959f05e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.9114080063103924\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     18118\n",
      "           1       0.73      0.27      0.40      2166\n",
      "\n",
      "    accuracy                           0.91     20284\n",
      "   macro avg       0.82      0.63      0.67     20284\n",
      "weighted avg       0.90      0.91      0.89     20284\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.9638138434233879\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     18118\n",
      "           1       0.84      0.82      0.83      2166\n",
      "\n",
      "    accuracy                           0.96     20284\n",
      "   macro avg       0.91      0.90      0.90     20284\n",
      "weighted avg       0.96      0.96      0.96     20284\n",
      "\n",
      "Naive Bayes:\n",
      "Accuracy: 0.8454939854072175\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     18118\n",
      "           1       0.31      0.36      0.33      2166\n",
      "\n",
      "    accuracy                           0.85     20284\n",
      "   macro avg       0.62      0.63      0.62     20284\n",
      "weighted avg       0.86      0.85      0.85     20284\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.9794912246105305\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     18118\n",
      "           1       0.99      0.82      0.89      2166\n",
      "\n",
      "    accuracy                           0.98     20284\n",
      "   macro avg       0.98      0.91      0.94     20284\n",
      "weighted avg       0.98      0.98      0.98     20284\n",
      "\n",
      "\n",
      "Comparison of Baseline Models:\n",
      "                     accuracy  precision    recall  f1-score  f1-score-binary\n",
      "Logistic Regression  0.911408   0.898611  0.911408  0.892843         0.396372\n",
      "Decision Tree        0.963814   0.963534  0.963814  0.963665         0.828984\n",
      "Naive Bayes          0.845494   0.856441  0.845494  0.850672         0.332339\n",
      "Random Forest        0.979491   0.979693  0.979491  0.978617         0.894790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to evaluate models and store results\n",
    "def evaluate_model(model, X_test, y_test, model_name, results):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1-score': report['weighted avg']['f1-score'],\n",
    "        'f1-score-binary': report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Baseline model 1: Logistic Regression\n",
    "evaluate_model(log_reg, X_test, y_test, \"Logistic Regression\", results)\n",
    "\n",
    "# Baseline model 2: Decision Tree\n",
    "evaluate_model(dec_tree, X_test, y_test, \"Decision Tree\", results)\n",
    "\n",
    "# Baseline model 3: Support Vector Machine\n",
    "# evaluate_model(svm, X_test, y_test, \"Support Vector Machine\", results)\n",
    "\n",
    "# Baseline model 4: Naive Bayes\n",
    "evaluate_model(nb, X_test, y_test, \"Naive Bayes\", results)\n",
    "\n",
    "# Baseline model 5: Random Forest\n",
    "evaluate_model(rf, X_test, y_test, \"Random Forest\", results)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nComparison of Baseline Models:\")\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BINpBO_d18i-",
    "outputId": "c26b6497-17e2-4a17-fb96-78366d02984e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9789489252612897\n",
      "KNN Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     18118\n",
      "           1       0.95      0.85      0.90      2166\n",
      "\n",
      "    accuracy                           0.98     20284\n",
      "   macro avg       0.97      0.92      0.94     20284\n",
      "weighted avg       0.98      0.98      0.98     20284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Baseline model 2: KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn_model = knn_model.predict(X_test)\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn_model))\n",
    "print(\"KNN Report:\\n\", classification_report(y_test, y_pred_knn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A77OKJT1pieZ",
    "outputId": "a00825d2-01be-4f29-9e3b-948a65c74015"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/.conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 100}\n",
      "Tuned Logistic Regression:\n",
      "Accuracy: 0.910422007493591\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     18118\n",
      "           1       0.71      0.28      0.40      2166\n",
      "\n",
      "    accuracy                           0.91     20284\n",
      "   macro avg       0.81      0.63      0.67     20284\n",
      "weighted avg       0.90      0.91      0.89     20284\n",
      "\n",
      "Best parameters for Logistic Regression: {'max_depth': 10}\n",
      "Tuned Decision Tree:\n",
      "Accuracy: 0.964602642476829\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     18118\n",
      "           1       0.94      0.72      0.81      2166\n",
      "\n",
      "    accuracy                           0.96     20284\n",
      "   macro avg       0.95      0.86      0.90     20284\n",
      "weighted avg       0.96      0.96      0.96     20284\n",
      "\n",
      "Best parameters for Logistic Regression: {'var_smoothing': 1e-09}\n",
      "Tuned Naive Bayes:\n",
      "Accuracy: 0.8454939854072175\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     18118\n",
      "           1       0.31      0.36      0.33      2166\n",
      "\n",
      "    accuracy                           0.85     20284\n",
      "   macro avg       0.62      0.63      0.62     20284\n",
      "weighted avg       0.86      0.85      0.85     20284\n",
      "\n",
      "Best parameters for Logistic Regression: {'max_depth': 30, 'n_estimators': 50}\n",
      "Tuned Random Forest:\n",
      "Accuracy: 0.9787517254979294\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     18118\n",
      "           1       0.99      0.81      0.89      2166\n",
      "\n",
      "    accuracy                           0.98     20284\n",
      "   macro avg       0.98      0.91      0.94     20284\n",
      "weighted avg       0.98      0.98      0.98     20284\n",
      "\n",
      "\n",
      "Comparison of Hyperparameter Tuned Models:\n",
      "                           accuracy  precision    recall  f1-score  \\\n",
      "Tuned Logistic Regression  0.910422   0.896551  0.910422  0.892388   \n",
      "Tuned Decision Tree        0.964603   0.963852  0.964603  0.962487   \n",
      "Tuned Naive Bayes          0.845494   0.856441  0.845494  0.850672   \n",
      "Tuned Random Forest        0.978752   0.978901  0.978752  0.977838   \n",
      "\n",
      "                           f1-score-binary  \n",
      "Tuned Logistic Regression         0.396947  \n",
      "Tuned Decision Tree               0.812140  \n",
      "Tuned Naive Bayes                 0.332339  \n",
      "Tuned Random Forest               0.890914  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "results = {}\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "param_grid_log_reg = {'C': [0.1, 1, 10, 100]}\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), param_grid_log_reg, cv=3, n_jobs=-1)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "best_log_reg = grid_log_reg.best_estimator_\n",
    "print(f\"Best parameters for Logistic Regression: {grid_log_reg.best_params_}\")\n",
    "evaluate_model(best_log_reg, X_test, y_test, \"Tuned Logistic Regression\", results)\n",
    "\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "param_grid_dec_tree = {'max_depth': [3, 5, 7, 10]}\n",
    "grid_dec_tree = GridSearchCV(DecisionTreeClassifier(), param_grid_dec_tree, cv=3, n_jobs=-1)\n",
    "grid_dec_tree.fit(X_train, y_train)\n",
    "best_dec_tree = grid_dec_tree.best_estimator_\n",
    "print(f\"Best parameters for Logistic Regression: {grid_dec_tree.best_params_}\")\n",
    "evaluate_model(best_dec_tree, X_test, y_test, \"Tuned Decision Tree\", results)\n",
    "\n",
    "# # Hyperparameter tuning for Support Vector Machine\n",
    "# param_grid_svm = {'C': [0.1, 10, 100], 'kernel': ['linear']}\n",
    "# grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=3, n_jobs=-1)\n",
    "# grid_svm.fit(X_train, y_train)\n",
    "# evaluate_model(grid_svm, X_test, y_test, \"Tuned SVM\", results)\n",
    "\n",
    "# Hyperparameter tuning for Naive Bayes\n",
    "param_grid_nb = {'var_smoothing': [1e-9, 1e-8, 1e-7]}\n",
    "grid_nb = GridSearchCV(GaussianNB(), param_grid_nb, cv=3, n_jobs=-1)\n",
    "grid_nb.fit(X_train, y_train)\n",
    "best_nb = grid_nb.best_estimator_\n",
    "print(f\"Best parameters for Logistic Regression: {grid_nb.best_params_}\")\n",
    "evaluate_model(best_nb, X_test, y_test, \"Tuned Naive Bayes\", results)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3, n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(f\"Best parameters for Logistic Regression: {grid_rf.best_params_}\")\n",
    "evaluate_model(best_rf, X_test, y_test, \"Tuned Random Forest\", results)\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df_grid = pd.DataFrame(results).T\n",
    "print(\"\\nComparison of Hyperparameter Tuned Models:\")\n",
    "print(results_df_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dtr70YA7STfj",
    "outputId": "502bdf60-2c3c-4d9d-eb25-fc6760f4bcff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as CSV file at: ../models/baseline_models_mit_raw.csv\n",
      "DataFrame saved as CSV file at: ../models/optimization_baseline_models_mit_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "results_df.to_csv(OutputFiles['model'], index=False)\n",
    "print(f\"DataFrame saved as CSV file at: {OutputFiles['model']}\")\n",
    "\n",
    "results_df_grid.to_csv(OutputFiles['Optimization'], index=False)\n",
    "print(f\"DataFrame saved as CSV file at: {OutputFiles['Optimization']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0GkxIW8s5GQ",
    "outputId": "6d590f19-4382-4af2-acb7-64266e06bdab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2024-12-01 09:46:05.144331\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Display the running time\n",
    "print(\"Current time:\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
