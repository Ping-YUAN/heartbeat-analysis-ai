{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess For PTB data clean\n",
        "\n",
        "Processing steps for PTB dataset: no resampling no scaler\n",
        "\n",
        "\n",
        "**Input** : raw data:  \n",
        "ptbdb_normal.csv  \n",
        "ptbdb_abnormal.csv  \n",
        "\n",
        "**Output** : clearn data:   \n",
        "ptb_train_clean.csv  \n",
        "ptb_test_clean.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local environment\n",
            "Current working directory: /Users/pingyuan/Documents/codeself/heartbeat-analysis-ai/notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "data_path = ''\n",
        "data_output_path = ''\n",
        "# Check if the environment is Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running on Google Colab\")\n",
        "    # Install required libraries\n",
        "    !pip install scikit-learn -q\n",
        "    !pip install pandas -q\n",
        "    !pip install numpy -q\n",
        "    !pip install imbalanced-learn -q\n",
        "\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # set the path where the csv file stored in your google drive. \n",
        "    data_path = '/content/drive/MyDrive/Heartbeat_Project/'\n",
        "    data_output_path = data_path\n",
        "    \n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "\n",
        "    current_path = os.getcwd()\n",
        "    print(\"Current working directory:\", current_path)\n",
        "    data_path = '../data/raw/'\n",
        "    data_output_path = '../data/processed/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_FSL5cYnEQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "RawFiles = dict({\n",
        "    'normal': data_path +  'ptbdb_normal.csv',\n",
        "    'abnormal': data_path +  'ptbdb_abnormal.csv',\n",
        "})\n",
        "\n",
        "OutputFiles = dict({\n",
        "    'test': data_output_path +  'ptb_test_clean.csv',\n",
        "    'train': data_output_path +  'ptb_train_clean.csv',\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hZhtCq2NnEQJ"
      },
      "outputs": [],
      "source": [
        "def addColumnsToDataframe(df):\n",
        "    \"\"\"\n",
        "    As the dataset is composed with 188 columns with the 188th columns as the category values,\n",
        "    so we give the last column the name 'target', others named with 'c_182'\n",
        "    \"\"\"\n",
        "    num_columns= df.shape[1]\n",
        "    feature_col_name = ['c_' + str(i) for i in range(0, num_columns - 1)]\n",
        "    df_columns = feature_col_name + ['target']\n",
        "    df.columns = df_columns\n",
        "    return df\n",
        "def convertColumnAsInt(df, column):\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce') # convert to numeric to handle NaN values\n",
        "    df.dropna(subset=[column], inplace=True)  # drop the rows with NaN values\n",
        "    df[column] = df[column].astype(int)  # convert to int\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ptb_normal = pd.read_csv(RawFiles.get('normal'), header=None)\n",
        "ptb_abnormal = pd.read_csv(RawFiles.get('abnormal'), header=None)\n",
        "\n",
        "ptb_data = pd.concat([ptb_normal, ptb_abnormal], axis=0, ignore_index=True)\n",
        "ptb_data = ptb_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "ptb_data = addColumnsToDataframe(ptb_data)  \n",
        "ptb_data = convertColumnAsInt(ptb_data, 'target') \n",
        "\n",
        "ptb_data = ptb_data.dropna(axis=0)\n",
        "y = ptb_data['target']\n",
        "X = ptb_data.drop(columns=['target'], inplace=False)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train shape (11641, 188)\n",
            "test shape (5230, 188)\n"
          ]
        }
      ],
      "source": [
        "# convert resampling rescaling data back to dataframe to concat\n",
        "X_train_df = pd.DataFrame(X_train, columns=[f'c_{i}' for i in range(X_train.shape[1])])\n",
        "X_test_df = pd.DataFrame(X_test, columns=[f'c_{i}' for i in range(X_test.shape[1])])\n",
        "y_train_df = pd.DataFrame(y_train, columns=['target'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['target'])\n",
        "\n",
        "# concat X_train, y_train(reset index to avoid join)\n",
        "ptb_train_clean = pd.concat(\n",
        "    [\n",
        "        X_train_df, y_train_df\n",
        "    ], axis=1)\n",
        "\n",
        "ptb_test_clean = pd.concat(\n",
        "    [  \n",
        "        X_test_df, y_test_df.reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "\n",
        "print('train shape', ptb_train_clean.shape)\n",
        "print('test shape', ptb_test_clean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save clean data to \n",
        "# ptb_train_clean.csv  \n",
        "# ptb_test_clean.csv\n",
        "\n",
        "ptb_train_clean.to_csv(OutputFiles.get('train'), index=False)\n",
        "ptb_test_clean.to_csv(OutputFiles.get('test'), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current time: 2024-11-07 11:59:24.850238\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "# Display the running time\n",
        "print(\"Current time:\", datetime.now())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
